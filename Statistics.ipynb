{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "At its core, statistics is about counting and measuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scales\n",
    "To count & measure effectively, we have to define scales on which to base our counts. A scale represents the possible values that a variable can have.\n",
    "\n",
    "Scales can be either discrete or continuous. Inches are on a continuous scale, and even fractions of an inch are possible. Half of a car isn't a meaningful quantity, because cars are discrete. You can't have 52% of a car - you either have a car, or you don't.\n",
    "You can still average items on discrete scales, though. You could say \"1.75 cars use this parking lot each day, on average.\" Any daily value for number of cars, however, would need to be a whole number\n",
    "\n",
    "\n",
    "* Equal Interval Scales Equal interval scales are always consistent.  No matter what speed you're traveling at, a difference of five miles per hour is always five miles per hour. The difference between 60 and 55 miles per hour will always be equal to the difference between 10 and five miles per hour.\n",
    "\n",
    "* Logarithmic Scales Each step on a logarithmic scale represents a different order of magnitude. The Richter scale that measures the strength of earthquakes, for example, is a logarithmic scale.\n",
    "\n",
    "* Ordinal scales where items are ordered by rank. how many cigarettes they smoke per day, and the answers could be \"none,\" \"a few,\" \"some,\" or \"a lot.\" These answers don't map exactly to numbers of cigarettes, but we know that \"a few\" is more than \"none.\"\n",
    "\n",
    "* Categorical scales Group values into general categories. One example is gender, which can be male or female. Unlike ordinal scales, categorical scales don't have an order. In our gender example, for instance, one category isn't greater than or less than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Zero \n",
    "Scales with absolute zero points that aren't at 0 don't enable us to take meaningful ratios. For example, if four cars parked in the lot yesterday and eight park today, I can safely say that twice as many cars are in the lot today.\n",
    "However, if it was 32 degrees Fahrenheit yesterday, and it's 64 degrees today, I can't say that it's twice as warm today as yesterday.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Data\n",
    "## Skew\n",
    "\n",
    "Skew refers to asymmetry in the data. When data is concentrated on the right side of the histogram, for example, we say it has a negative skew. When the data is concentrated on the left, we say it has a positive skew. We can measure the level of skew with the skew function. A positive value indicates a positive skew, a negative value indicates a negative skew, and a value close to zero indicates no skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_scores_positive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-911c4dfd6120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpositive_skew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores_positive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnegative_skew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores_negative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mno_skew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_scores_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_scores_positive' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "positive_skew = skew(test_scores_positive)\n",
    "negative_skew = skew(test_scores_negative)\n",
    "no_skew = skew(test_scores_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurtosis:\n",
    "\n",
    "Kurtosis measures whether the distribution is short and flat, or tall and skinny. In other words, it assesses the shape of the peak. \"Shorter\" distributions have a lower maximum frequency, but higher subsequent frequencies. A high kurtosis may indicate problems with outliers (very large or very small values that skew the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can measure kurtosis with the kurtosis function.\n",
    "# Negative values indicate platykurtic distributions, positive values indicate leptokurtic distributions, and values near 0 are mesokurtic.\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "kurt_platy = kurtosis(test_scores_platy)\n",
    "kurt_lepto = kurtosis(test_scores_lepto)\n",
    "kurt_meso = kurtosis(test_scores_meso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modality: \n",
    "\n",
    "Modality refers to the number of modes, or peaks, in a distribution.\n",
    "Real-world data is often unimodal (it has only one mode). More than one peak means that the plot is multimodal. We can't easily measure the modality of a plot, like we can with kurtosis or skew.\n",
    "Often, the best way to detect multimodality is to examine the plot visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central tendency measures: Mean\n",
    "the mathematical symbol for the mean:\n",
    "Î¼ x\n",
    "\n",
    "This symbol means \"the average of all of the values in x.\" The fact that x is lowercase and in bold indicates that it's a vector. Very large and very small values can easily skew the mean.\n",
    "Very skewed distributions can make the mean misleading.\n",
    "If we subtract the mean of a set of numbers from each of the numbers within that set, the overall total of all of the differences will always add up to zero.\n",
    "That's because the mean is the \"center\" of the data. All of the differences that are negative will always cancel out all of the differences that are positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central tendency measures: Median \n",
    "\n",
    "Another measure of central tendency. This is the midpoint of an array.\n",
    "\n",
    "To calculate the median, we need to sort the array, then take the value in the middle. If there are two values in the middle (because there are an even number of items in the array), then we take the mean of the two middle values.\n",
    "\n",
    "The median is less sensitive to very large or very small values (which we call outliers), and is a more realistic center of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee68a56a1e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmedian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvalues_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalues_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues_median\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmedian_difference_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy import median\n",
    "\n",
    "values_median = numpy.median(values)\n",
    "values_list = [value - values_median for value in values]\n",
    "median_difference_sum = sum(values_list)\n",
    "\n",
    "\n",
    "plt.hist(test_scores_positive)\n",
    "median = numpy.median(test_scores_positive)\n",
    "plt.axvline(median, color = \"g\")\n",
    "plt.axvline(test_scores_positive.mean(), color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread: Variance\n",
    "\n",
    "Variance tells us how concentrated or \"spread out\" the data is around the mean. We calculate variance by subtracting every value from the mean, squaring the results, and then averaging them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread: Standard Deviation\n",
    "\n",
    "Most common way to refer to the distance between data points and the mean. It's a very useful concept and a great way to measure the density of a data set.\n",
    "While it may sound complicated, standard deviation is fairly straightforward; it's the square root of the variance.\n",
    "\n",
    "A Deviation is the distance a single data value is from the mean. Standard Deviation is finding an âaverageâ of sorts across the dataset.\n",
    "\n",
    "Statisticians and data scientists typically measure the percentage of data that falls within one or two standard deviations of the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate standard deviation by using the std() method on a pandas series.\n",
    "std_dev = nba_stats[\"pf\"].std()\n",
    "\n",
    "# We can calculate how many standard deviations a data point is from the mean by doing some subtraction and division.\n",
    "# First, we find the total distance by subtracting the mean.\n",
    "total_distance = nba_stats[\"pf\"][0] - mean\n",
    "# Then we divide by standard deviation to find how many standard deviations away the point is.\n",
    "standard_deviation_distance = total_distance / std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread: Normal distribution\n",
    " \n",
    "A special kind of distribution. You might recognize it more commonly as a bell curve.\n",
    "The normal distribution is found in a variety of natural phenomena. If we made a histogram of the heights of everyone on the planet, for example, it would be more or less a normal distribution.\n",
    "\n",
    "One cool thing about normal distributions is that for every single one, the same percentage of the data is within one standard deviation of the mean, the same percentage is within two standard deviations of the mean, and so on.\n",
    "About 68% of the data is within one standard deviation, roughly 95% is within two standard deviations, and about 99% is within three standard deviations.\n",
    "\n",
    "We can generate a normal distribution by using a probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arange function generates a numpy vector\n",
    "# The vector below will start at -1, and go up to, but not including 1\n",
    "# It will proceed in \"steps\" of .01.  So the first element will be -1, the second -.99, the \tthird -.98, all the way up to .99.\n",
    "points = np.arange(-1, 1, 0.01)\n",
    "\n",
    "# The norm.pdf function will take the points vector and convert it into a probability vector\n",
    "# Each element in the vector will correspond to the normal distribution (earlier elements \tand later element smaller, peak in the center)\n",
    "# The distribution will be centered on 0, and will have a standard devation of .3\n",
    "probabilities = norm.pdf(points, 0, .3)\n",
    "\n",
    "# Plot the points values on the x-axis and the corresponding probabilities on the y-axis\n",
    "plt.plot(points, probabilities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread: Covariance \n",
    "\n",
    "Comparing two variables/data sets together to determine how these different numbers vary jointly. \n",
    "A set of variables can't co-vary more from the mean than they individually vary from the mean. Two variables reach the maximum possible covariance when they vary in an identical way (ie, you see a straight line on the plot).\n",
    "The r-value is a ratio between the actual covariance and the maximum possible positive covariance\n",
    "ï¿¼\n",
    "ï¿¼For the denominator, we need to multiply the standard deviations for x and y. This is the maximum possible positive covariance, which is just both of the standard deviation values multiplied. If we divide our actual covariance by this, we get the r-value.\n",
    "\n",
    "\n",
    "âThe problem with covariances is that they are hard to compare: when you calculate the covariance of a set of heights and weights, as expressed in (respectively) meters and kilograms, you will get a different covariance from when you do it in other units (which already gives a problem for people doing the same thing with or without the metric system!), but also, it will be hard to tell if (e.g.) height and weight 'covariate better' than, e.g. the length of your toes and fingers, simply because the 'scale' you calculate the covariance on is different.\n",
    "\n",
    "\n",
    "The solution to this is to 'normalize' the covariance: you divide the covariance by something that represents the diversity and scale in both the covariates, and end up with a value that is assured to be between -1 and 1: the correlation. Whatever unit your original variables were in, you will always get the same result, and this will also ensure that you can, to a certain degree, compare whether two variables 'correlate' more than two others, simply by comparing their correlation.â\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Spread: Correlation\n",
    "\n",
    "The most common way to measure correlation is to use Pearson's r, which we also call an r-value. An r-value ranges from -1 to 1, and indicates how strongly two variables are correlated.\n",
    "\n",
    "* A 1 indicates a perfect positive correlation. This would appear as a straight, upward-sloping line on our plots.\n",
    "* A 0 indicates no correlation. We'd see a scatterplot with points that appear random.\n",
    "* A -1 indicates a perfect negative correlation. This would appear as a straight, downward-sloping line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "r_value, p_value = pearsonr(fandango_ratings, mc_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squares, cubes, square root etc:\n",
    "\n",
    "** 2 calculates the equivalent of variable * variable\n",
    "** 3 calculates the equivalent of variable * variable * variable\n",
    "The same pattern holds true as we raise to higher powers like 4, 5, and so on.\n",
    "\n",
    "We can also take the roots of numbers using the same syntax.\n",
    "difference ** (1/2) will take the square root. We need to put the fraction in parentheses because raising a value to a power is the operation that would normally occur first.\n",
    "difference ** (1/3) will take the cube root. We can even continue with even smaller fractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression:\n",
    "\n",
    "Used to make predictions about wine quality using existing data: Linear regression gives us a formula. If we plug in the value for one variable into this formula, we get the value for the other variable.\n",
    "The equation to create the formula takes the form:\n",
    "**y = mx + b**\n",
    "\n",
    "This equation is saying \"the predicted value of the second variable (y) is equal to the value of the first variable (x) times the slope (m) plus the intercept (b)\".\n",
    "\n",
    "**m** = the covariance of x and y divided by the variance of x.\n",
    "\n",
    "We can use the cov function to calculate covariance, and the .var() method on Pandas \tseries to calculate variance.\n",
    "\n",
    "**b** = The intercept is just how much higher or lower the average y point is than our \tpredicted value.\n",
    "\n",
    "We can compute the intercept by take the mean of the y values, and then subtract the \tslope times the mean of the x values from that.\n",
    "\n",
    "Now that we've computed our slope and our intercept, we can make predictions about the y-values from the x-values.\n",
    "\n",
    "In order to do this, we go back to our original formula: y = mx + b and just plug in the values for m and b\n",
    "\n",
    "We can then compute predicted y-values for any x-value. This lets us make predictions about the quality of x-values that we've never seen. For example, a wine with a density of .98 isn't in our dataset, but we can make a prediction about what quality a reviewer would assign to a wine with this density.\n",
    "\n",
    "Depending on how correlated the predictor and the value being predicted are, the predictions may be good or bad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_slope(x, y):\n",
    "    return cov(x, y)[0, 1] / x.var()\n",
    "\n",
    "def calc_intercept(x, y, slope):\n",
    "    return y.mean() - (slope * x.mean())\n",
    "\n",
    "def predict_y_value(x):\n",
    "    return (slope * x) + intercept\n",
    "\n",
    "# OR linregress function \n",
    "\n",
    "slope, intercept, r_value, p_value, stderr_slope = linregress(wine_quality[\"density\"], \twine_quality[\"quality\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to figure out if our regression is good or bad.\n",
    "\n",
    "We can plot out our line and our actual values, and see how far apart they are on the y-axis.\n",
    "We can also compute the distance between each prediction and the actual value -- these distances are called residuals.\n",
    "\n",
    "If we add up the sum of the squared residuals, we can get a good error estimate for our line.\n",
    "We have to add the squared residuals, because just like differences from the mean, the residuals add to 0 if they aren't squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = slope * wine_quality[\"density\"] + intercept\n",
    "rss = sum((wine_quality[\"quality\"] - predicted_y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error:\n",
    "\n",
    "The standard error is similar to the standard deviation, but it tries to make an estimate for the whole population of y-values -- even the ones we haven't seen yet that we may want to predict in the future.\n",
    "\n",
    "The standard error lets us quickly determine how good or bad a linear model is at prediction.\n",
    "From the sum of squared residuals.\n",
    "ï¿¼\n",
    "You take the sum of squared residuals, divide by the number of y-points minus two, and then take the square root.\n",
    "\n",
    "You might be wondering about why 2 is subtracted -- this is due to differences between the whole population and a sample. This will be explained in more depth later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = np.asarray([slope * x + intercept for x in wine_quality[\"density\"]])\n",
    "residuals = (wine_quality[\"quality\"] - predicted_y) ** 2\n",
    "rss = sum(residuals)\n",
    "\n",
    "# calculate the standard error\n",
    "std_err = (rss / (len(wine_quality[\"quality\"]) -2)) ** 0.5\n",
    "\n",
    "def calc_proportion(y, predicted_y, std_err, count):\n",
    "    error_range = std_err * count\n",
    "    \n",
    "    # create a series of the differences between actual and predicted values\n",
    "    differences = abs(y - predicted_y)\n",
    "    \n",
    "    # for each value in the series check if its within the error range\n",
    "    y_in_error_range = [value for value in differences if value <= error_range]\n",
    "    \n",
    "    # return number of differences as a propotion of total values\n",
    "    return len(y_in_error_range) /len(y)\n",
    "\n",
    "within_one = calc_proportion(wine_quality[\"quality\"], predicted_y, std_err, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Random Sequence:\n",
    "\n",
    "A random seed is an integer that is used to \"seed\" a random number generator. After a random seed is set, the numbers generated after will follow the same sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed of 20 and generate a list of 10 random numbers between the values 0 and 10\n",
    "random.seed(20)\n",
    "new_sequence = [random.randint(0,10) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Random Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping = [300, 200, 100, 600, 20]\n",
    "\n",
    "# We want to sample the data, and only select 4 elements.\n",
    "random.seed(1)\n",
    "\n",
    "shopping_sample = random.sample(shopping, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sample from a pandas dataframe:\n",
    "    \n",
    "dataframe.sample method (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html). Arguably more powerful and versatile than the generic python.sample method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope is 0.0973110779739 intercept is 3.7997739189\n",
      "r_value is 0.178449190739 p_value is 0.0311615162285\n",
      "stderr is  0.0447135446568\n",
      "Rating of 1 on Metacritic predicted as Fandango rating 3.89708499687\n",
      "Rating of 3 on Metacritic predicted as Fandango rating 4.09170715282\n",
      "Rating of 5 on Metacritic predicted as Fandango rating 4.28632930877\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYGaWcU4EZmYZ9/8BFU03sZaDS00AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0ce95cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "movies = pd.read_csv(\"data/fandango_score_comparison.csv\")\n",
    "mc_ratings = movies[\"Metacritic_norm_round\"]\n",
    "fandango_ratings = movies[\"Fandango_Stars\"]\n",
    "\n",
    "# Use the scipy.stats.linregress function create a linear regression with Metacritic_norm_round as the x-values and Fandango_Stars as the y-values.\n",
    "from scipy.stats import linregress\n",
    "\n",
    "slope, intercept, r_value, p_value, stderr_slope = linregress(mc_ratings, fandango_ratings)\n",
    "\n",
    "print(\"Slope is\", slope, \"intercept is\", intercept)\n",
    "print(\"r_value is\", r_value,\"p_value is\", p_value) \n",
    "print(\"stderr is \", stderr_slope)\n",
    "\n",
    "# Predict what a movie that got a 3.0 in Metacritic would get on Fandango \n",
    "prediction_1 = 1 * slope + intercept\n",
    "print(\"Rating of 1 on Metacritic predicted as Fandango rating\", prediction_1)\n",
    "\n",
    "# Predict what a movie that got a 3.0 in Metacritic would get on Fandango \n",
    "prediction_3 = 3 * slope + intercept\n",
    "print(\"Rating of 3 on Metacritic predicted as Fandango rating\", prediction_3)\n",
    "\n",
    "# Predict what a movie that got a 3.0 in Metacritic would get on Fandango \n",
    "prediction_5 = 5 * slope + intercept\n",
    "print(\"Rating of 5 on Metacritic predicted as Fandango rating\", prediction_5)\n",
    "\n",
    "# Create a residual plot to see how the linear regression line relates to existing datapoints\n",
    "plt.scatter(mc_ratings, fandango_ratings)\n",
    "x_values = [1.0, 3.0, 5.0]\n",
    "y_values = [prediction_1, prediction_3, prediction_5]\n",
    "plt.plot(x_values, y_values, color=\"red\")\n",
    "plt.xlim(1,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
