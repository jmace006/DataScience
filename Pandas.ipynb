{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe from CSV ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "recent_grads = pd.read_csv(\"data/recent-grads.csv\")\n",
    "print(type(recent_grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58ba4380186f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# turn a dictionary into a dataframe to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/submission_1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'submission_df' is not defined"
     ]
    }
   ],
   "source": [
    "# turn a dictionary into a dataframe to csv\n",
    "submission = pd.DataFrame(submission_df)\n",
    "submission.to_csv(\"data/submission_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the DataFrame ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank  Major_code                           Major   Total     Men  Women  \\\n",
      "0     1        2419           PETROLEUM ENGINEERING  2339.0  2057.0  282.0   \n",
      "1     2        2416  MINING AND MINERAL ENGINEERING   756.0   679.0   77.0   \n",
      "2     3        2415       METALLURGICAL ENGINEERING   856.0   725.0  131.0   \n",
      "\n",
      "  Major_category  ShareWomen  Sample_size  Employed      ...        Part_time  \\\n",
      "0    Engineering    0.120564           36      1976      ...              270   \n",
      "1    Engineering    0.101852            7       640      ...              170   \n",
      "2    Engineering    0.153037            3       648      ...              133   \n",
      "\n",
      "   Full_time_year_round  Unemployed  Unemployment_rate  Median  P25th   P75th  \\\n",
      "0                  1207          37           0.018381  110000  95000  125000   \n",
      "1                   388          85           0.117241   75000  55000   90000   \n",
      "2                   340          16           0.024096   73000  50000  105000   \n",
      "\n",
      "   College_jobs  Non_college_jobs  Low_wage_jobs  \n",
      "0          1534               364            193  \n",
      "1           350               257             50  \n",
      "2           456               176              0  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "Rank                      int64\n",
      "Major_code                int64\n",
      "Major                    object\n",
      "Total                   float64\n",
      "Men                     float64\n",
      "Women                   float64\n",
      "Major_category           object\n",
      "ShareWomen              float64\n",
      "Sample_size               int64\n",
      "Employed                  int64\n",
      "Full_time                 int64\n",
      "Part_time                 int64\n",
      "Full_time_year_round      int64\n",
      "Unemployed                int64\n",
      "Unemployment_rate       float64\n",
      "Median                    int64\n",
      "P25th                     int64\n",
      "P75th                     int64\n",
      "College_jobs              int64\n",
      "Non_college_jobs          int64\n",
      "Low_wage_jobs             int64\n",
      "dtype: object\n",
      "RangeIndex(start=0, stop=173, step=1)\n"
     ]
    }
   ],
   "source": [
    "dimensions = recent_grads.shape\n",
    "num_rows = dimensions[0]\n",
    "num_cols = dimensions[1]\n",
    "first_twenty = recent_grads.head(20)\n",
    "\n",
    "print(recent_grads.head(3))\n",
    "print(recent_grads.dtypes)\n",
    "print(recent_grads.index)\n",
    "\n",
    "col_names = recent_grads.columns.tolist()\n",
    "null_counts = recent_grads.isnull().sum()\n",
    "\n",
    "recent_grads.mean()\n",
    "\n",
    "recent_grads.describe()\n",
    "\n",
    "\n",
    "#Quick data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "recent_grads.hist()\n",
    "plt.show()\n",
    "\n",
    "star_wars[star_wars.columns[9:15]].mean().plot.bar()\n",
    "star_wars[star_wars.columns[3:9]].sum().plot.bar()\n",
    "males[males.columns[9:15]].mean().plot.bar(title=\"Male Star Wars Films Ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add column names to a Dataset on Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [‘col1’,'col2’,'col3']\n",
    "\n",
    "# read in the dataset, assigning column names as per the documentation\n",
    "cars = pd.read_csv('imports-85.data', names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic name creation\n",
    "for col in star_wars.columns[9:15]:\n",
    "    star_wars = star_wars.rename(columns={col:\"ranking_\"+str(counter)})\n",
    "    counter = counter + 1\n",
    "\n",
    "# or if all cols known\n",
    "star_wars = star_wars.rename(columns={\n",
    "     'Unnamed 15': \"Han Solo\",\n",
    " 'Unnamed: 16': \"Luke Skywalker\",\n",
    " 'Unnamed: 17': \"Princess Leia Organa\",\n",
    " 'Unnamed: 18': \"Anakin Skywalker\",\n",
    "     'Unnamed: 28': \"Yoda\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a row ##\n",
    "NOTE: Unlike slicing lists in Python, a slice of a dataframe using .loc[] will include both the start and the end row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hundredth_row = recent_grads.loc[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting multiple rows ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows 3, 4, 5 and 6\")\n",
    "print(recent_grads.loc[3:6])\n",
    "\n",
    "print(\"Rows 2, 5, and 10\")\n",
    "two_five_ten = [2,5,10]\n",
    "print(recent_grads.loc[two_five_ten])\n",
    "\n",
    "num_rows = recent_grads.shape[0]\n",
    "last_rows = recent_grads[num_rows-5:num_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting individual columns ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_info = pd.read_csv(\"data/food_info.csv\")\n",
    "saturated_fat = food_info[\"FA_Sat_(g)\"]\n",
    "cholesterol = food_info[\"Cholestrl_(mg)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting multiple columns by name ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_copper = food_info[[\"Zinc_(mg)\", \"Copper_(mg)\"]]\n",
    "\n",
    "columns = [\"Zinc_(mg)\", \"Copper_(mg)\"]\n",
    "zinc_copper = food_info[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Unique Values from Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ages[“major_cat”].unique()\n",
    "\n",
    "categories = transactions.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Method\n",
    "df_filtered = df.query('salary>30000')\n",
    "df_filtered = df.query('A == \"bar\"')\n",
    "\n",
    "# add new column with a 1 if true, 0 if false\n",
    "df['isalone'] = np.where((df['SibSp']==0)&(df['Parch']==0),1,0)\n",
    "\n",
    "# Instead of queries, we can use in-dices. We do that by using an array index with boolean expressions:\n",
    "rows = all_ages[all_ages[“cat”]==“value”]]\n",
    "\n",
    "df_filtered = df[(df.salary >= 30000) & (df.year == 2017)]\n",
    "\n",
    "# View Rows Where Coverage Is Greater Than 50\n",
    "df[df['coverage'] > 50]\n",
    "\n",
    "# View Rows Where Coverage Is Greater Than 50 And Reports Less Than 4\n",
    "df[(df['coverage']  > 50) & (df['reports'] < 4)]\n",
    "\n",
    "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Cohort\"]=='2006']\n",
    "data[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Demographic\"]=='Total Cohort']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the data frame\n",
    "By default, pandas will sort the data by the column we specify in ascending order and return a new DataFrame, rather than modifying food_info itself. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_info.sort_values(\"Sodium_(mg)\")\n",
    "\n",
    "# Sorts the DataFrame in-place, rather than returning a new DataFrame.\n",
    "food_info.sort_values(\"Sodium_(mg)\", inplace=True)\n",
    "\n",
    "# Sorts by descending order, rather than ascending.\n",
    "food_info.sort_values(\"Sodium_(mg)\", inplace=True, ascending=False)\n",
    "\n",
    "# Or by column name\n",
    "food_info.sort(\"Norm_Nutr_Index\",ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex (eg. after sorting)\n",
    "By default, retains the old index by adding an extra column to the dataframe with the old index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_reindexed = new_titanic_survival.reset_index(drop=True)\n",
    "subset = titanic_reindexed.iloc[0:5,0:3]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=146, step=1)\n",
      "Index(['Avengers: Age of Ultron (2015)', 'Cinderella (2015)', 'Ant-Man (2015)',\n",
      "       'Do You Believe? (2015)', 'Hot Tub Time Machine 2 (2015)',\n",
      "       'The Water Diviner (2015)', 'Irrational Man (2015)', 'Top Five (2014)',\n",
      "       'Shaun the Sheep Movie (2015)', 'Love & Mercy (2015)',\n",
      "       ...\n",
      "       'The Woman In Black 2 Angel of Death (2015)', 'Danny Collins (2015)',\n",
      "       'Spare Parts (2015)', 'Serena (2015)', 'Inside Out (2015)',\n",
      "       'Mr. Holmes (2015)', ''71 (2015)', 'Two Days, One Night (2014)',\n",
      "       'Gett: The Trial of Viviane Amsalem (2015)',\n",
      "       'Kumiko, The Treasure Hunter (2015)'],\n",
      "      dtype='object', name='FILM', length=146)\n"
     ]
    }
   ],
   "source": [
    "fandango = pd.read_csv(\"data/fandango_score_comparison.csv\")\n",
    "print(fandango.index)\n",
    "\n",
    "## Using Integer Indexes to Select Rows ##\n",
    "first_last = fandango.iloc[[0,fandango.shape[0]-1]]\n",
    "\n",
    "## Using Custom Indexes ##\n",
    "fandango_films = fandango.set_index('FILM',inplace=False,drop=False)\n",
    "print(fandango_films.index)\n",
    "\n",
    "## Using a Custom Index for Selection ##\n",
    "movies = [\"The Lazarus Effect (2015)\",\"Gett: The Trial of Viviane Amsalem (2015)\",\"Mr. Holmes (2015)\"]\n",
    "best_movies_ever = fandango_films.loc[movies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns of a certain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = fandango_films.dtypes\n",
    "\n",
    "# filter data types to just floats, index attributes returns just column names\n",
    "float_columns = types[types.values == 'float64'].index\n",
    "\n",
    "# use bracket notation to filter columns to just float columns\n",
    "float_df = fandango_films[float_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select rows of a certain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_df = df.select_dtypes(include=['float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Apply function on Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metacritic_User               1.505529\n",
      "IMDB                          0.955447\n",
      "Fandango_Stars                0.538532\n",
      "Fandango_Ratingvalue          0.501106\n",
      "RT_norm                       1.503265\n",
      "RT_user_norm                  0.997787\n",
      "Metacritic_norm               0.972522\n",
      "Metacritic_user_nom           0.752765\n",
      "IMDB_norm                     0.477723\n",
      "RT_norm_round                 1.509404\n",
      "RT_user_norm_round            1.003559\n",
      "Metacritic_norm_round         0.987561\n",
      "Metacritic_user_norm_round    0.785412\n",
      "IMDB_norm_round               0.501043\n",
      "Fandango_Difference           0.152141\n",
      "dtype: float64\n",
      "                                Metacritic_User  IMDB  Fandango_Stars  \\\n",
      "FILM                                                                    \n",
      "Avengers: Age of Ultron (2015)             14.2  15.6            10.0   \n",
      "\n",
      "                                Fandango_Ratingvalue  RT_norm  RT_user_norm  \\\n",
      "FILM                                                                          \n",
      "Avengers: Age of Ultron (2015)                   9.0      7.4           8.6   \n",
      "\n",
      "                                Metacritic_norm  Metacritic_user_nom  \\\n",
      "FILM                                                                   \n",
      "Avengers: Age of Ultron (2015)              6.6                  7.1   \n",
      "\n",
      "                                IMDB_norm  RT_norm_round  RT_user_norm_round  \\\n",
      "FILM                                                                           \n",
      "Avengers: Age of Ultron (2015)        7.8            7.0                 9.0   \n",
      "\n",
      "                                Metacritic_norm_round  \\\n",
      "FILM                                                    \n",
      "Avengers: Age of Ultron (2015)                    7.0   \n",
      "\n",
      "                                Metacritic_user_norm_round  IMDB_norm_round  \\\n",
      "FILM                                                                          \n",
      "Avengers: Age of Ultron (2015)                         7.0              8.0   \n",
      "\n",
      "                                Fandango_Difference  \n",
      "FILM                                                 \n",
      "Avengers: Age of Ultron (2015)                  1.0  \n",
      "                                Metacritic_User  IMDB  Fandango_Stars  \\\n",
      "FILM                                                                    \n",
      "Avengers: Age of Ultron (2015)             3.55   3.9             2.5   \n",
      "\n",
      "                                Fandango_Ratingvalue  RT_norm  RT_user_norm  \\\n",
      "FILM                                                                          \n",
      "Avengers: Age of Ultron (2015)                  2.25     1.85          2.15   \n",
      "\n",
      "                                Metacritic_norm  Metacritic_user_nom  \\\n",
      "FILM                                                                   \n",
      "Avengers: Age of Ultron (2015)             1.65                1.775   \n",
      "\n",
      "                                IMDB_norm  RT_norm_round  RT_user_norm_round  \\\n",
      "FILM                                                                           \n",
      "Avengers: Age of Ultron (2015)       1.95           1.75                2.25   \n",
      "\n",
      "                                Metacritic_norm_round  \\\n",
      "FILM                                                    \n",
      "Avengers: Age of Ultron (2015)                   1.75   \n",
      "\n",
      "                                Metacritic_user_norm_round  IMDB_norm_round  \\\n",
      "FILM                                                                          \n",
      "Avengers: Age of Ultron (2015)                        1.75              2.0   \n",
      "\n",
      "                                Fandango_Difference  \n",
      "FILM                                                 \n",
      "Avengers: Age of Ultron (2015)                 0.25  \n"
     ]
    }
   ],
   "source": [
    "double_df = float_df.apply(lambda x: x*2)\n",
    "print(double_df.head(1))\n",
    "\n",
    "halved_df = float_df.apply(lambda x: x/2)\n",
    "print(halved_df.head(1))\n",
    "\n",
    "def hundredth_row(column):\n",
    "    hundredth_item = column.iloc[99]\n",
    "    return hundredth_item\n",
    "\n",
    "hundredth_row_var = titanic_survival.apply(hundredth_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Apply function on Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILM\n",
      "Avengers: Age of Ultron (2015)    0.375\n",
      "Cinderella (2015)                 0.125\n",
      "Ant-Man (2015)                    0.225\n",
      "Do You Believe? (2015)            0.925\n",
      "Hot Tub Time Machine 2 (2015)     0.150\n",
      "dtype: float64\n",
      "FILM\n",
      "Avengers: Age of Ultron (2015)    3.925\n",
      "Cinderella (2015)                 3.875\n",
      "Ant-Man (2015)                    4.275\n",
      "Do You Believe? (2015)            3.275\n",
      "Hot Tub Time Machine 2 (2015)     1.550\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rt_mt_user = float_df[['RT_user_norm', 'Metacritic_user_nom']]\n",
    "rt_mt_deviations = rt_mt_user.apply(lambda x: np.std(x), axis=1)\n",
    "print(rt_mt_deviations[0:5])\n",
    "\n",
    "rt_mt_means = rt_mt_user.apply(lambda x: np.mean(x), axis=1) \n",
    "print(rt_mt_means.head(5))\n",
    "\n",
    "def is_minor(row):\n",
    "    if pd.isnull(row[\"age\"]):\n",
    "        return \"unknown\"\n",
    "    elif row[\"age\"] < 18:\n",
    "        return \"minor\"\n",
    "    elif row[\"age\"] >=18:\n",
    "        return \"adult\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "age_labels= titanic_survival.apply(is_minor, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Series/index\n",
    "When we select a single column from a DataFrame, pandas will return the Series object representing that column. By default, pandas indexes each individual Series object in a DataFrame with the integer data type. The Series object uses 0-indexing. \n",
    "\n",
    "With both NumPy arrays and Series objects, we can pass integer indexes into bracket notation to slice and select values. With Series objects, however, we can also specify custom indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avengers: Age of Ultron (2015)' 'Cinderella (2015)' 'Ant-Man (2015)'\n",
      " 'Do You Believe? (2015)' 'Hot Tub Time Machine 2 (2015)'\n",
      " 'The Water Diviner (2015)' 'Irrational Man (2015)' 'Top Five (2014)'\n",
      " 'Shaun the Sheep Movie (2015)' 'Love & Mercy (2015)'\n",
      " 'Far From The Madding Crowd (2015)' 'Black Sea (2015)' 'Leviathan (2014)'\n",
      " 'Unbroken (2014)' 'The Imitation Game (2014)' 'Taken 3 (2015)'\n",
      " 'Ted 2 (2015)' 'Southpaw (2015)'\n",
      " 'Night at the Museum: Secret of the Tomb (2014)' 'Pixels (2015)'\n",
      " 'McFarland, USA (2015)' 'Insidious: Chapter 3 (2015)'\n",
      " 'The Man From U.N.C.L.E. (2015)' 'Run All Night (2015)'\n",
      " 'Trainwreck (2015)' 'Selma (2014)' 'Ex Machina (2015)'\n",
      " 'Still Alice (2015)' 'Wild Tales (2014)' 'The End of the Tour (2015)'\n",
      " 'Red Army (2015)' 'When Marnie Was There (2015)'\n",
      " 'The Hunting Ground (2015)' 'The Boy Next Door (2015)' 'Aloha (2015)'\n",
      " 'The Loft (2015)' '5 Flights Up (2015)' 'Welcome to Me (2015)'\n",
      " 'Saint Laurent (2015)' 'Maps to the Stars (2015)'\n",
      " \"I'll See You In My Dreams (2015)\" 'Timbuktu (2015)' 'About Elly (2015)'\n",
      " 'The Diary of a Teenage Girl (2015)' 'Kingsman: The Secret Service (2015)'\n",
      " 'Tomorrowland (2015)' 'The Divergent Series: Insurgent (2015)'\n",
      " 'Annie (2014)' 'Fantastic Four (2015)' 'Terminator Genisys (2015)'\n",
      " 'Pitch Perfect 2 (2015)' 'Entourage (2015)' 'The Age of Adaline (2015)'\n",
      " 'Hot Pursuit (2015)' 'The DUFF (2015)' 'Black or White (2015)'\n",
      " 'Project Almanac (2015)' 'Ricki and the Flash (2015)' 'Seventh Son (2015)'\n",
      " 'Mortdecai (2015)' 'Unfinished Business (2015)' 'American Ultra (2015)'\n",
      " 'True Story (2015)' 'Child 44 (2015)' 'Dark Places (2015)'\n",
      " 'Birdman (2014)' 'The Gift (2015)' 'Unfriended (2015)'\n",
      " 'Monkey Kingdom (2015)' 'Mr. Turner (2014)'\n",
      " 'Seymour: An Introduction (2015)' 'The Wrecking Crew (2015)'\n",
      " 'American Sniper (2015)' 'Furious 7 (2015)'\n",
      " 'The Hobbit: The Battle of the Five Armies (2014)' 'San Andreas (2015)'\n",
      " 'Straight Outta Compton (2015)' 'Vacation (2015)' 'Chappie (2015)'\n",
      " 'Poltergeist (2015)' 'Paper Towns (2015)' 'Big Eyes (2014)'\n",
      " 'Blackhat (2015)' 'Self/less (2015)' 'Sinister 2 (2015)'\n",
      " 'Little Boy (2015)' 'Me and Earl and The Dying Girl (2015)'\n",
      " 'Maggie (2015)' 'Mad Max: Fury Road (2015)' 'Spy (2015)'\n",
      " 'The SpongeBob Movie: Sponge Out of Water (2015)' 'Paddington (2015)'\n",
      " 'Dope (2015)' 'What We Do in the Shadows (2015)' 'The Overnight (2015)'\n",
      " 'The Salt of the Earth (2015)' 'Song of the Sea (2014)'\n",
      " 'Fifty Shades of Grey (2015)' 'Get Hard (2015)' 'Focus (2015)'\n",
      " 'Jupiter Ascending (2015)' 'The Gallows (2015)'\n",
      " 'The Second Best Exotic Marigold Hotel (2015)' 'Strange Magic (2015)'\n",
      " 'The Gunman (2015)' 'Hitman: Agent 47 (2015)' 'Cake (2015)'\n",
      " 'The Vatican Tapes (2015)' 'A Little Chaos (2015)'\n",
      " 'The 100-Year-Old Man Who Climbed Out the Window and Disappeared (2015)'\n",
      " 'Escobar: Paradise Lost (2015)' 'Into the Woods (2014)'\n",
      " 'It Follows (2015)' 'Inherent Vice (2014)' 'A Most Violent Year (2014)'\n",
      " \"While We're Young (2015)\" 'Clouds of Sils Maria (2015)'\n",
      " 'Testament of Youth (2015)' 'Infinitely Polar Bear (2015)'\n",
      " 'Phoenix (2015)' 'The Wolfpack (2015)'\n",
      " 'The Stanford Prison Experiment (2015)' 'Tangerine (2015)'\n",
      " 'Magic Mike XXL (2015)' 'Home (2015)' 'The Wedding Ringer (2015)'\n",
      " 'Woman in Gold (2015)' 'The Last Five Years (2015)'\n",
      " 'Mission: Impossible â€“ Rogue Nation (2015)' 'Amy (2015)'\n",
      " 'Jurassic World (2015)' 'Minions (2015)' 'Max (2015)'\n",
      " 'Paul Blart: Mall Cop 2 (2015)' 'The Longest Ride (2015)'\n",
      " 'The Lazarus Effect (2015)' 'The Woman In Black 2 Angel of Death (2015)'\n",
      " 'Danny Collins (2015)' 'Spare Parts (2015)' 'Serena (2015)'\n",
      " 'Inside Out (2015)' 'Mr. Holmes (2015)' \"'71 (2015)\"\n",
      " 'Two Days, One Night (2014)' 'Gett: The Trial of Viviane Amsalem (2015)'\n",
      " 'Kumiko, The Treasure Hunter (2015)']\n",
      "Avengers: Age of Ultron (2015)                     74\n",
      "Cinderella (2015)                                  85\n",
      "Ant-Man (2015)                                     80\n",
      "Do You Believe? (2015)                             18\n",
      "Hot Tub Time Machine 2 (2015)                      14\n",
      "The Water Diviner (2015)                           63\n",
      "Irrational Man (2015)                              42\n",
      "Top Five (2014)                                    86\n",
      "Shaun the Sheep Movie (2015)                       99\n",
      "Love & Mercy (2015)                                89\n",
      "Far From The Madding Crowd (2015)                  84\n",
      "Black Sea (2015)                                   82\n",
      "Leviathan (2014)                                   99\n",
      "Unbroken (2014)                                    51\n",
      "The Imitation Game (2014)                          90\n",
      "Taken 3 (2015)                                      9\n",
      "Ted 2 (2015)                                       46\n",
      "Southpaw (2015)                                    59\n",
      "Night at the Museum: Secret of the Tomb (2014)     50\n",
      "Pixels (2015)                                      17\n",
      "McFarland, USA (2015)                              79\n",
      "Insidious: Chapter 3 (2015)                        59\n",
      "The Man From U.N.C.L.E. (2015)                     68\n",
      "Run All Night (2015)                               60\n",
      "Trainwreck (2015)                                  85\n",
      "Selma (2014)                                       99\n",
      "Ex Machina (2015)                                  92\n",
      "Still Alice (2015)                                 88\n",
      "Wild Tales (2014)                                  96\n",
      "The End of the Tour (2015)                         92\n",
      "                                                 ... \n",
      "Clouds of Sils Maria (2015)                        89\n",
      "Testament of Youth (2015)                          81\n",
      "Infinitely Polar Bear (2015)                       80\n",
      "Phoenix (2015)                                     99\n",
      "The Wolfpack (2015)                                84\n",
      "The Stanford Prison Experiment (2015)              84\n",
      "Tangerine (2015)                                   95\n",
      "Magic Mike XXL (2015)                              62\n",
      "Home (2015)                                        45\n",
      "The Wedding Ringer (2015)                          27\n",
      "Woman in Gold (2015)                               52\n",
      "The Last Five Years (2015)                         60\n",
      "Mission: Impossible â€“ Rogue Nation (2015)        92\n",
      "Amy (2015)                                         97\n",
      "Jurassic World (2015)                              71\n",
      "Minions (2015)                                     54\n",
      "Max (2015)                                         35\n",
      "Paul Blart: Mall Cop 2 (2015)                       5\n",
      "The Longest Ride (2015)                            31\n",
      "The Lazarus Effect (2015)                          14\n",
      "The Woman In Black 2 Angel of Death (2015)         22\n",
      "Danny Collins (2015)                               77\n",
      "Spare Parts (2015)                                 52\n",
      "Serena (2015)                                      18\n",
      "Inside Out (2015)                                  98\n",
      "Mr. Holmes (2015)                                  87\n",
      "'71 (2015)                                         97\n",
      "Two Days, One Night (2014)                         97\n",
      "Gett: The Trial of Viviane Amsalem (2015)         100\n",
      "Kumiko, The Treasure Hunter (2015)                 87\n",
      "Length: 146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fandango = pd.read_csv('data/fandango_score_comparison.csv')\n",
    "series_film = fandango[\"FILM\"]\n",
    "series_rt = fandango[\"RottenTomatoes\"]\n",
    "\n",
    "from pandas import Series\n",
    "\n",
    "film_names = series_film.values\n",
    "rt_scores = series_rt.values\n",
    "print(film_names)\n",
    "\n",
    "series_custom = Series(data=rt_scores,index=film_names)\n",
    "print(series_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Integer Index Preservation ##\n",
    "\n",
    "Even though we specified that the Series object uses a custom string index, the object still has an internal integer index that we can use for selection. When it comes to indexes, Series objects act like both dictionaries and lists. We can access values with our custom index (like the keys in a dictionary), or the integer index (like the index in a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Water Diviner (2015)             63\n",
      "Irrational Man (2015)                42\n",
      "Top Five (2014)                      86\n",
      "Shaun the Sheep Movie (2015)         99\n",
      "Love & Mercy (2015)                  89\n",
      "Far From The Madding Crowd (2015)    84\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "series_custom = Series(rt_scores , index=film_names)\n",
    "series_custom[['Minions (2015)', 'Leviathan (2014)']]\n",
    "fiveten = series_custom.iloc[5:11]\n",
    "print(fiveten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindexing a Series ##\n",
    "Reindexing to specify another order for labels in a series object.  Reindexing is the pandas way of modifying the alignment between labels (indexes) and the data (values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the original index\n",
    "original_index = series_custom.index\n",
    "\n",
    "# Sort the index with sorted() & reindex() to set the newly-ordered index.\n",
    "sorted_by_index = series_custom.reindex(sorted(original_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting a Series ##\n",
    "sort_index() method that sorts a Series by index, and a sort_values() method that sorts a Series by its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2 = series_custom.sort_index()\n",
    "sc3 = series_custom.sort_values()\n",
    "print(sc2.head(10))\n",
    "print(sc3.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Transforming ##\n",
    "(use any of the standard Python arithmetic operators (+, -, *, and /) to transform each of the values in a Series object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_custom/10\n",
    "\n",
    "# Add each value with each other\n",
    "np.add(series_custom, series_custom)\n",
    "\n",
    "# Apply sine function to each value\n",
    "np.sin(series_custom)\n",
    "\n",
    "# Return the highest value (will return a single value, not a Series)\n",
    "np.max(series_custom)\n",
    "\n",
    "series_normalized = series_custom/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Comparing and Filtering ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_one = series_custom > 50\n",
    "criteria_two = series_custom < 75\n",
    "\n",
    "both_criteria = series_custom[criteria_one & criteria_two]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Alignment ##\n",
    "Series objects, pandas implicitly preserves the link between the index labels and the values across operations and transformations, unless we explicitly break it. Offers a big advantage over using NumPy objects. For Series objects in particular, this means we can use the standard Python arithmetic operators (+, -, *, and /) to add, subtract, multiply, and divide the values at each index label for two different Series objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_critics = Series(fandango['RottenTomatoes'].values, index=fandango['FILM'])\n",
    "rt_users = Series(fandango['RottenTomatoes_User'].values, index=fandango['FILM'])\n",
    "\n",
    "rt_mean = (rt_critics + rt_users)/2\n",
    "\n",
    "print(rt_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming a Column ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_1000 = food_info[\"Iron_(mg)\"] / 1000\n",
    "add_100 = food_info[\"Iron_(mg)\"] + 100\n",
    "sub_100 = food_info[\"Iron_(mg)\"] - 100\n",
    "mult_2 = food_info[\"Iron_(mg)\"]*2\n",
    "sodium_grams = food_info[\"Sodium_(mg)\"] / 1000\n",
    "sugar_milligrams = food_info[\"Sugar_Tot_(g)\"] * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unrate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-21a83412d646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munrate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unrate' is not defined"
     ]
    }
   ],
   "source": [
    "unrate[\"DATE\"] = pd.to_datetime(unrate[\"DATE\"])\n",
    "\n",
    "# Convert to numeric:\n",
    "cols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n",
    "\n",
    "for col in cols:\n",
    "\n",
    "    data[\"ap_2010\"][col] = pd.to_numeric(data[\"ap_2010\"][col], errors=\"coerce\")\n",
    "    \n",
    "print(data[\"ap_2010\"].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming with Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_energy = food_info[\"Water_(g)\"] * food_info[\"Energ_Kcal\"]\n",
    "print(water_energy[0:5])\n",
    "\n",
    "grams_of_protein_per_gram_of_water  = food_info[\"Protein_(g)\"] / food_info[\"Water_(g)\"]\n",
    "milligrams_of_calcium_and_iron = food_info[\"Calcium_(mg)\"] + food_info[\"Iron_(mg)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(food_info[\"Protein_(g)\"][0:5])\n",
    "max_protein = food_info[\"Protein_(g)\"].max()\n",
    "print(max_protein)\n",
    "\n",
    "max_protein = food_info[\"Protein_(g)\"].max()\n",
    "normalized_protein = food_info[\"Protein_(g)\"] / max_protein\n",
    "\n",
    "max_lipid = food_info[\"Lipid_Tot_(g)\"].max()\n",
    "normalized_fat = food_info[\"Lipid_Tot_(g)\"] / max_lipid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a New Column ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_protein = (food_info[\"Protein_(g)\"] - food_info[\"Protein_(g)\"].min()) / (food_info[\"Protein_(g)\"].max() - food_info[\"Protein_(g)\"].min())\n",
    "normalized_fat = (food_info[\"Lipid_Tot_(g)\"] - food_info[\"Lipid_Tot_(g)\"].min()) / (food_info[\"Lipid_Tot_(g)\"].max() - food_info[\"Lipid_Tot_(g)\"].min())\n",
    "\n",
    "food_info[\"Normalized_Protein\"] = normalized_protein\n",
    "food_info[\"Normalized_Fat\"] = normalized_fat\n",
    "\n",
    "food_info[\"Norm_Nutr_Index\"] = (2 * food_info[\"Normalized_Protein\"]) - (0.75 * food_info[\"Normalized_Fat\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of Nulls in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "titanic_survival = pd.read_csv(\"data/titanic_survival.csv\")\n",
    "\n",
    "age = titanic_survival[\"age\"]\n",
    "\n",
    "age_is_null = pd.isnull(age)\n",
    "age_null_true = age[age_is_null]\n",
    "age_null_count = age_null_true.shape[0]\n",
    "print(age_null_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of Nulls in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = titanic_survival.isnull().sum()\n",
    "\n",
    "# Then select only those columns without nulls from the original dataset:\n",
    "df_no_nulls = titanic_survival[null_counts[null_counts==0].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show valid/invalid records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15     NaN\n",
      "37     NaN\n",
      "40     NaN\n",
      "46     NaN\n",
      "59     NaN\n",
      "69     NaN\n",
      "70     NaN\n",
      "74     NaN\n",
      "80     NaN\n",
      "106    NaN\n",
      "107    NaN\n",
      "108    NaN\n",
      "118    NaN\n",
      "121    NaN\n",
      "125    NaN\n",
      "134    NaN\n",
      "147    NaN\n",
      "152    NaN\n",
      "157    NaN\n",
      "166    NaN\n",
      "176    NaN\n",
      "179    NaN\n",
      "184    NaN\n",
      "196    NaN\n",
      "204    NaN\n",
      "219    NaN\n",
      "223    NaN\n",
      "235    NaN\n",
      "237    NaN\n",
      "241    NaN\n",
      "        ..\n",
      "1212   NaN\n",
      "1213   NaN\n",
      "1214   NaN\n",
      "1215   NaN\n",
      "1216   NaN\n",
      "1219   NaN\n",
      "1221   NaN\n",
      "1241   NaN\n",
      "1242   NaN\n",
      "1243   NaN\n",
      "1245   NaN\n",
      "1246   NaN\n",
      "1247   NaN\n",
      "1249   NaN\n",
      "1250   NaN\n",
      "1253   NaN\n",
      "1255   NaN\n",
      "1262   NaN\n",
      "1268   NaN\n",
      "1282   NaN\n",
      "1283   NaN\n",
      "1284   NaN\n",
      "1291   NaN\n",
      "1292   NaN\n",
      "1293   NaN\n",
      "1297   NaN\n",
      "1302   NaN\n",
      "1303   NaN\n",
      "1305   NaN\n",
      "1309   NaN\n",
      "Name: age, Length: 264, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "age_is_null = pd.isnull(titanic_survival[\"age\"])\n",
    "invalid_records = titanic_survival[\"age\"][age_is_null]\n",
    "valid_records = titanic_survival[\"age\"][age_is_null==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select rows where col is not null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = star_wars[pd.notnull(star_wars[\"RespondentID\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean of all numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in Missing Data\n",
    "\n",
    "Note that if a column consists entirely of null or NaN values, pandas won't be able to fill in the missing values when we use the df.fillna() method along with the df.mean() method, because there won't be a mean.\n",
    "\n",
    "We should fill any NaN or null values that remain after the initial replacement with the value 0. We can do this by passing 0 into the df.fillna() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_mean_age = titanic_survival[\"age\"].mean()\n",
    "correct_mean_fare = titanic_survival[\"fare\"].mean()\n",
    "\n",
    "# Only select float columns from the df with missing values\n",
    "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
    "\n",
    "# Return a data frame with missing values replaced with mean of that column.\n",
    "float_cols = float_cols.fillna(float_cols.mean())\n",
    "\n",
    "# fill with zero\n",
    "float_cols = float_cols.fillna(o)\n",
    "\n",
    "# check for missing values\n",
    "print(float_cols.isnull().sum())\n",
    "\n",
    "#This method will replace any missing values in a dataframe with the values we specify. \n",
    "means = df.mean()\n",
    "df = df.fillna(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace data frame values with a mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_length is the column to map in, can be more than one\n",
    "mapping_dict = {\n",
    "    \"emp_length\": {\n",
    "        \"10+ years\": 10,\n",
    "        \"9 years\": 9,\n",
    "        \"8 years\": 8,\n",
    "        \"n/a\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "loans = loans.replace(mapping_dict)\n",
    "\n",
    "# another more verbose method\n",
    "yes_no = {\n",
    "    \"Yes\": True,\n",
    "    \"No\": False\n",
    "}\n",
    "\n",
    "have_you_seen= \"Have you seen any of the 6 films in the Star Wars franchise?\"\n",
    "do_you_consider = \"Do you consider yourself to be a fan of the Star Wars film franchise?\"\n",
    "\n",
    "star_wars[have_you_seen] = star_wars[have_you_seen].map(yes_no)\n",
    "star_wars[do_you_consider] = star_wars[do_you_consider].map(yes_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Missing Values ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_na_rows = titanic_survival.dropna(axis=0)\n",
    "drop_na_columns = titanic_survival.dropna(axis=1)\n",
    "\n",
    "new_titanic_survival = titanic_survival.dropna(axis=0,subset=[\"age\",\"sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "Easy way to subset/group by one column and then apply a calculation like a sum or a mean. Pivot tables first group and then apply a calculation. \n",
    "-  index tells the method which column to group by. \n",
    "- values is the column that we want to apply the calculation to, and \n",
    "- aggfunc specifies the calculation we want to perform. The default for the aggfunc parameter is mean if left out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "passenger_survival = titanic_survival.pivot_table(index=\"pclass\", values=\"survived\")\n",
    "passenger_age = titanic_survival.pivot_table(index=\"pclass\",values=\"age\")\n",
    "\n",
    "# Calculation on multiple columns at once (pass as a list):\n",
    "port_stats = titanic_survival.pivot_table(index=\"embarked\",values=[\"fare\",\"survived\"],aggfunc=np.sum)\n",
    "\n",
    "age_group_survival = titanic_survival.pivot_table(index=\"age_labels\",values=\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = class_size.groupby(\"DBN\").agg(np.mean)\n",
    "class_size.reset_index(inplace=True)\n",
    "data['class_size'] = class_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Rows by Position ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already sorted new_titanic_survival by age\n",
    "first_five_rows = new_titanic_survival.iloc[0:5]\n",
    "first_ten_rows = new_titanic_survival.iloc[0:10]\n",
    "row_position_fifth = new_titanic_survival.iloc[4]\n",
    "row_index_25 = new_titanic_survival.loc[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Rows and Columns by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_first_column = new_titanic_survival.iloc[0,0]\n",
    "all_rows_first_three_columns = new_titanic_survival.iloc[:,0:3]\n",
    "row_index_83_age = new_titanic_survival.loc[83,\"age\"]\n",
    "row_index_766_pclass = new_titanic_survival.loc[766,\"pclass\"]\n",
    "\n",
    "row_index_1100_age = new_titanic_survival.loc[1100,\"age\"]\n",
    "row_index_25_survived = new_titanic_survival.loc[25,\"survived\"]\n",
    "five_rows_three_cols = new_titanic_survival.iloc[0:5,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Index/Reindex Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_reindexed = new_titanic_survival.reset_index(drop=True)\n",
    "subset = titanic_reindexed.iloc[0:5,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_train = train.select_dtypes(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Specific Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['PID', 'Year Built', 'Year Remod/Add', 'Garage Yr Blt', 'Mo Sold', 'Yr Sold']\n",
    "numerical_train = train.drop(cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns with nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sum of nulls in each column:\n",
    "train_null_counts = train.isnull().sum()\n",
    "\n",
    "# Then select only those columns without nulls from the original dataset:\n",
    "df_no_nulls = train[train_null_counts[train_null_counts==0].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform categories to a numerical representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Utilities'] = train['Utilities'].astype('category')\n",
    "\n",
    "# to access the numerical representation:\n",
    "train['Utilities'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummies for numerical Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = pd.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all numerical categories to dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in text_cols:\n",
    "\n",
    "    dummies = pd.get_dummies(train[col])\n",
    "    train = pd.concat([train, dummies], axis=1)\n",
    "    \n",
    "    # Delete the original columns from text_cols from the train data frame.\n",
    "    train = train.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change column values into groups of values\n",
    "we can separate this continuous feature into a categorical feature by dividing it into range:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_age(df,cut_points,label_names):\n",
    "    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
    "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "    return df\n",
    "\n",
    "cut_points = [-1,0,18,100]\n",
    "label_names = [\"Missing\",\"Child\",\"Adult\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of instances of a value in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0ac2d50184d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get a series representing the ‘main dish…’ column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_dishes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'What is typically the main dish at your Thanksgiving dinner?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# count number of instances of each response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dishes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# get a series representing the ‘main dish…’ column\n",
    "main_dishes = data['What is typically the main dish at your Thanksgiving dinner?']\n",
    "\n",
    "# count number of instances of each response\n",
    "print(main_dishes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Correlations between Dataframe Columns\n",
    "\n",
    "We can use the pandas pandas.DataFrame.corr() method to find correlations between columns in a dataframe. \n",
    "The method returns a new dataframe where the index for each column and row is the name of a column in the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined is our dataframe\n",
    "correlations = combined.corr()\n",
    "correlations = correlations[\"sat_score\"]\n",
    "print(correlations)\n",
    "\n",
    "# visually compare columns that indicate a correlation\n",
    "import matplotlib.pyplot as plt\n",
    "combined.plot.scatter(x='total_enrollment', y='sat_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining/Merging Dataframes\n",
    "\n",
    "Using the pandas pandas.DataFrame.merge() function. The \"left\" dataframe is the one we call the method on, and the \"right\" dataframe is the one we pass into df.merge(). Because we're using the DBN column to join the dataframes, we'll need to specify the keyword argument on=\"DBN\" when calling pandas.DataFrame..merge()\n",
    "\n",
    "- With an inner merge, we'd only combine rows where the same DBN exists in both data sets.\n",
    "- With a left merge, we'd only use DBN values from the dataframe on the \"left\" of the merge.\n",
    "- With a right merge, we'll only use DBN values from the dataframe on the \"right\" of the merge.\n",
    "- With an outer merge, we'll take any DBN values from either sat_results or class_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = data[\"sat_results\"]\n",
    "\n",
    "combined = combined.merge(data[\"ap_2010\"],how=\"left\", on=\"DBN\")\n",
    "combined = combined.merge(data[\"graduation\"],how=\"left\", on=\"DBN\")\n",
    "\n",
    "datasets = ['class_size', 'demographics', 'survey', 'hs_directory']\n",
    "for dataset in datasets:\n",
    "    combined = combined.merge(data[dataset], on=\"DBN\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstabs:\n",
    "\n",
    "The crosstab function will print a table that shows frequency counts for two or more columns. Here's how you could use the pandas.crosstab function:\n",
    "…bit like a big group by!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "table = pandas.crosstab(income[\"sex\"], [income[\"high_income\"]])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window (Time Series)\n",
    "\n",
    "Pandas has some time series tools that can help, including the rolling_mean function, which will do most of the hard computation for you. Set the window equal to the number of trading days in the past you want to use to compute the indicators. This will add in NaN values for any row where there aren't enough historical trading days to do the computation. \n",
    "\n",
    "Note: There is a giant caveat here, which is that the rolling mean will use the current day's price. You'll need to reindex the resulting series to shift all the values \"forward\" one day. For example, the rolling mean calculated for 1950-01-03 will need to be assigned to 1950-01-04, and so on. You can use the shift method on Dataframes to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data = pd.read_csv('data/sphist.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values(by=['Date'],ascending=True)\n",
    "\n",
    "# calculate some indicators to add to the data, start with mean()\n",
    "\n",
    "data['day_5_mean'] = 0\n",
    "\n",
    "# use a rolling window of 5 days over the 'Close' column to get the mean, then shift the values forward by one day so that the mean doesnt include that days value\n",
    "data['day_5_mean'] = data['Close'].rolling(5).mean()\n",
    "data['day_5_mean'] = data['day_5_mean'].shift()\n",
    "\n",
    "data['day_30_mean'] = 0\n",
    "data['day_30_mean'] = data['Close'].rolling(30).mean()\n",
    "data['day_30_mean'] = data['day_30_mean'].shift()\n",
    "\n",
    "data['day_365_mean'] = 0\n",
    "data['day_365_mean'] = data['Close'].rolling(365).mean()\n",
    "data['day_365_mean'] = data['day_365_mean'].shift()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
