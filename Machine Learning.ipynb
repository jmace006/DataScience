{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "\n",
    "Similarity metrics works by comparing a fixed set of numerical features, another word for attributes, between 2 observations, or living spaces in our case. \n",
    "When trying to predict a continuous value, like price, the main similarity metric that's used is Euclidean distance. Here's the general formula for Euclidean distance:\n",
    "\n",
    "* The lowest value you can achieve is 0. This happens when the value for the feature is exactly the same for both observations you're comparing.\n",
    "* Euclidean distance equation expects numerical values\n",
    "* expects a value for each observation and attribute:\n",
    "    * remove columns: dc_listings = dc_listings.drop(drop_columns, axis=1) \n",
    "    * remove rows with missing data: dc_listings = dc_listings.dropna(axis = 0, subset=null_columns)\n",
    "    * check: print(dc_listings.isnull().sum())\n",
    "* ranking by Euclidean distance doesn't make sense if all attributes aren't ordinal\n",
    "\n",
    "\n",
    "can instead use the distance.euclidean() function from scipy.spatial, which takes in 2 vectors as the parameters and calculates the Euclidean distance between them. The euclidean() function expects:\n",
    "* both of the vectors to be represented using a list-like object (Python list, NumPy array, or pandas Series)\n",
    "* both of the vectors must be 1-dimensional and have the same number of elements\n",
    "Here's a simple example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "first_listing = [-0.596544, -0.439151]\n",
    "second_listing = [-0.596544, 0.412923]\n",
    "dist = distance.euclidean(first_listing, second_listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation/Standardization\n",
    "To prevent any single column from having too much of an impact on the distance, we can normalize all of the columns to have a mean of 0 and a standard deviation of 1.\n",
    "Normalizing the values in each columns to the standard normal distribution (mean of 0, standard deviation of 1) preserves the distribution of the values in each column while aligning the scales. To normalize the values in a column to the standard normal distribution, you need to:\n",
    "* from each value, subtract the mean of the column\n",
    "* divide each value by the standard deviation of the column\n",
    "\n",
    "NOTE: These methods were written with mass column transformation in mind and when you call mean() or std(), the appropriate column means and column standard deviations are used for each value in the Dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract each value in the column by the mean.\n",
    "first_transform = dc_listings['maximum_nights'] - dc_listings['maximum_nights'].mean()\n",
    "\n",
    "# Divide each value in the column by the standard deviation.\n",
    "normalized_col = first_transform / first_transform.std()\n",
    "\n",
    "# To apply this transformation across all of the columns in a Dataframe, you can use the corresponding Dataframe methods mean() and std():\n",
    "normalized_listings = (dc_listings - dc_listings.mean()) / (dc_listings.std())\n",
    "\n",
    "# Another example of Normalizing the data\n",
    "pga['distance'] = (pga['distance'] - pga['distance'].mean()) / pga['distance'].std()\n",
    "pga['accuracy'] = (pga['accuracy'] - pga['accuracy'].mean()) / pga['accuracy'].std()\n",
    "print(pga.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "\n",
    "Most popular machine learning in Python. \n",
    "Scikit-learn contains functions for all of the major machine learning algorithms and a simple, unified workflow. \n",
    "Both of these properties allow data scientists to be incredibly productive when training and testing different models on a new dataset.\n",
    "\n",
    "The scikit-learn workflow consists of 4 main steps:\n",
    "1. instantiate the specific machine learning model you want to use\n",
    "2. fit the model to the training data\n",
    "3. use the model to make predictions\n",
    "4. evaluate the accuracy of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instansiate the model\n",
    "\n",
    "Scikit-learn uses a similar object-oriented style to Matplotlib and you need to instantiate an empty model first by calling the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. fit the model to the data: \n",
    "using the fit method. For all models, the fit method takes in 2 required parameters:\n",
    "matrix-like object, containing the feature columns we want to use from the training set.\n",
    "list-like object, containing correct target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split full dataset into train and test sets.\n",
    "train_df = normalized_listings.iloc[0:2792]\n",
    "test_df = normalized_listings.iloc[2792:]\n",
    "\n",
    "# Matrix-like object, containing just the 2 columns of interest from training set.\n",
    "train_features = train_df[['accommodates', 'bathrooms']]\n",
    "\n",
    "# List-like object, containing just the target column, `price`.\n",
    "train_target = normalized_listings['price']\n",
    "\n",
    "# Pass everything into the fit method.\n",
    "knn.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Make Predictions\n",
    "\n",
    "We can use the predict method to make predictions on the test set. The predict method has only one required parameter, matrix-like object, containing the feature columns from the dataset we want to make predictions on.\n",
    "\n",
    "The number of feature columns you use during both training and testing need to match or scikit-learn will return an error\n",
    "\n",
    "The predict() method returns a NumPy array containing the predicted price values for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(test_df[['accommodates', 'bathrooms']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating Accuracy\n",
    "\n",
    "Always use cross-validation to make sure the error metrics you are getting from your model are accurate. The most common form of cross validation, and the one we will be using, is called k-fold cross validation:\n",
    "\n",
    "gives an accuracy score\n",
    "\n",
    "cv is the number of folds - a bit like iterations\n",
    "￼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "lr = LogisticRegression()\n",
    "scores = cross_val_score(lr, all_X, all_y, cv=10)\n",
    "accuracy = np.mean(scores)\n",
    "\n",
    "print(scores,accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Metrics\n",
    "\n",
    "a metric that quantifies how good the predictions were on the test set - quantifies how inaccurate our predictions were from the actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean error:\n",
    "\n",
    "calculating the difference between each predicted and actual value and then averaging these differences - isn't an effective error metric for most cases. Mean error treats a positive difference differently than a negative difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean absolute error:\n",
    "\n",
    "compute the absolute value of each error before we average all the errors. ￼\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(train[features], train[target])\n",
    "predictions = lr.predict(test[features])\n",
    "mae = mean_absolute_error(test[target],predictions)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean squared error (MSE) :\n",
    "\n",
    "Take the mean of the squared error values, makes the gap between the predicted and actual values more clear. A prediction that's off by 100 dollars will have an error (of 10,000) that's 100 times more than a prediction that's off by only 10 dollars (which will have an error of 100).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the actual values with the predicted values\n",
    "mean_squared_error(test_one['price'],predicted_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root mean squared error (RMSE):\n",
    "\n",
    "Error metric whose units are the base unit, RMSE for short, this error metric is calculated by taking the square root of the MSE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,algorithm='auto')\n",
    "knn.fit(train_one[['accommodates']],train_one['price'])\n",
    "predicted_price = knn.predict(test_one[['accommodates']])\n",
    "iteration_one_rmse = (mean_squared_error(test_one['price'],predicted_price)) ** (1/2)\n",
    "\n",
    "# or\n",
    "test_rmse = np.sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE to RMSE:\n",
    "\n",
    "In general, we should expect that the RMSE value be much less than the MAE value because we're taking the square root of the squared errors. . Looking at the ratio of MAE to RMSE can help us understand if there are large but infrequent errors. You can read more about comparing MAE and RMSE in this wonderful post.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters.:\n",
    "\n",
    "Values that affect the behavior and performance of a model that are unrelated to the data that's used. he process of finding the optimal hyperparameter value is known as hyperparameter optimization. A simple but common hyperparameter optimization technique is known as grid search, which involves:\n",
    "* selecting a subset of the possible hyperparameter values,\n",
    "* training a model using each of these hyperparameter values,\n",
    "* evaluating each model's performance,\n",
    "* selecting the hyperparameter value that resulted in the lowest error value.\n",
    "\n",
    "\n",
    "Grid search essentially boils down to evaluating the model performance at different k values and selecting the k value that resulted in the lowest error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = [x for x in range(1,21)]\n",
    "mse_values = list()\n",
    "feautures =  ['accommodates', 'bedrooms', 'bathrooms', 'beds','minimum_nights', 'maximum_nights', 'number_of_reviews']\n",
    "\n",
    "for i in hyper_params:\n",
    "   \n",
    "    knn = KNeighborsRegressor(n_neighbors=i,algorithm='brute')\n",
    "    knn.fit(train_df[features],train_df['price'])\n",
    "    predictions = knn.predict(test_df[features])\n",
    "    mse_values.append(mean_squared_error(test_df['price'],predictions))\n",
    "    \n",
    "plt.scatter(hyper_params,mse_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout validation \n",
    "\n",
    "Involves:\n",
    "* splitting the full dataset into 2 partitions:\n",
    "    * a training set\n",
    "    * a test set\n",
    "* training the model on the training set,\n",
    "* using the trained model to predict labels on the test set,\n",
    "* computing an error metric to understand the model's effectiveness,\n",
    "* switch the training and test sets and repeat,\n",
    "* average the errors.\n",
    "\n",
    "\n",
    "In holdout validation, we usually use a 50/50 split instead of the 75/25 split from train/test validation. This way, we remove number of observations as a potential source of variation in our model performance.\n",
    "specific example of a larger class of validation techniques called k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Value\n",
    "The number of similar records to compare to, can use similarity metric (below) to find those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Different K Values ##\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "num_folds = [3, 5, 7, 9, 10, 11, 13, 15, 17, 19, 21, 23]\n",
    "\n",
    "for fold in num_folds:\n",
    "    kf = KFold(fold, shuffle=True, random_state=1)\n",
    "    model = KNeighborsRegressor()\n",
    "    mses = cross_val_score(model, dc_listings[[\"accommodates\"]], dc_listings[\"price\"], scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "    rmses = np.sqrt(np.absolute(mses))\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    std_rmse = np.std(rmses)\n",
    "    \n",
    "    print(str(fold), \"folds: \", \"avg RMSE: \", str(avg_rmse), \"std RMSE: \", str(std_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "\n",
    "takes advantage of a larger proportion of the data during training while still rotating through different subsets of the data to avoid the issues of train/test validation.\n",
    "Here's the algorithm from k-fold cross validation:\n",
    "* splitting the full dataset into k equal length partitions,\n",
    "    * selecting k-1 partitions as the training set and\n",
    "    * selecting the remaining partition as the test set\n",
    "* training the model on the training set,\n",
    "* using the trained model to predict labels on the test fold,\n",
    "* computing the test fold's error metric,\n",
    "* repeating all of the above steps k-1 times, until each partition has been used as the test set for an iteration,\n",
    "* calculating the mean of the k error values.\n",
    "\n",
    "\n",
    "Holdout validation is essentially a version of k-fold cross validation when k is equal to 2. Generally, 5 or 10 folds is used for k-fold cross-validation. Here's a diagram describing each iteration of 5-fold cross validation:\n",
    "As you increase the number the folds, the number of observations in each fold decreases and the variance of the fold-by-fold errors increases. Let's start by manually partitioning the data set into 5 folds. Instead of splitting into 5 dataframes, let's add a column that specifies which fold the row belongs to. This way, we can easily select\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold class from sklearn.model_selection:\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits, shuffle=False, random_state=None)\n",
    "\n",
    "# n_splits is the number of folds you want to use,\n",
    "# shuffle is used to toggle shuffling of the ordering of the observations in the dataset,\n",
    "# random_state is used to specify the random seed value if shuffle is set to True.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits, shuffle=True, random_state=1)\n",
    "model = KNeighborsRegressor()\n",
    "mses = cross_val_score(model,dc_listings[['accommodates']],dc_listings['price'],scoring=\"neg_mean_squared_error\", cv=kf)\n",
    "rmses = np.sqrt(np.absolute(mses))\n",
    "avg_rmse = np.mean(rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Val Score\n",
    "\n",
    "You'll notice here that no parameters depend on the data set at all. This is because the KFold class returns an iterator object which we use in conjunction with the cross_val_score() function, also from sklearn.model_selection. Together, these 2 functions allow us to compactly train and test using k-fold cross validation.\n",
    "\n",
    "Depending on the scoring criteria you specify, either a single total value is returned one value for each fold. Here's the general workflow for performing k-fold cross-validation using the classes we just described:\n",
    "* instantiate the scikit-learn model class you want to fit,\n",
    "* instantiate the KFold class and using the parameters to specify the k-fold cross-validation attributes you want,\n",
    "* use the cross_val_score() function to return the scoring metric you're interested in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(estimator, X, Y, scoring=None, cv=None)\n",
    "\n",
    "# estimator is a sklearn model that implements the fit method (e.g. instance of KNeighborsRegressor),\n",
    "# X is the list or 2D array containing the features you want to train on,\n",
    "# y is a list containing the values you want to predict (target column),\n",
    "# scoring is a string describing the scoring criteria (list of accepted values here).\n",
    "# cv describes the number of folds. Here are some examples of accepted values:\n",
    "    # an instance of the KFold class,\n",
    "    # an integer representing the number of folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias & Variance\n",
    "\n",
    "Bias and variance are the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance. In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff.\n",
    "\n",
    "\n",
    "** The standard deviation of the RMSE is a proxy for a model's variance**\n",
    "\n",
    "** The average RMSE is a proxy for a model's bias. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear function\n",
    "\n",
    "The word linear equation is often used interchangeably with linear function. Many real world processes can be modeled using multiple, related linear equations. \n",
    "\n",
    "\n",
    "A simple, straight line is more clearly defined as a linear function. All linear functions can be written in the following form:\n",
    "y = mx + b\n",
    "For a specific linear function, m and b are constant values while x and y are variables.\n",
    "y = 3x + 1 and y = 5 are both examples of linear function.\n",
    "\n",
    "\n",
    "When m is equal to 0, the line is completely flat and is parallel to the x-axis. \n",
    "When m and b are both set to 0, the line is equivalent to the x-axis.\n",
    "The m value controls a line's slope while the b value controls a line's y-intercept. \n",
    "Another way to think about slope is rate of change. The rate of change is how much the y axis changes for a specific change in the x axis.\n",
    "\n",
    "\n",
    "or m = (y1-y2) / (x1/x2)\n",
    "\n",
    "When x1and x2 are equivalent,the slope value is undefined (This is because division by 0 has no meaning in mathematical calculations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Linear Functions:\n",
    "\n",
    "Whenever x is raised to a power not equal to 1 , we have a non-linear function. \n",
    "When we calculate the slope between 2 points on a curve, we're really calculating the slope between the line that intersects both of those points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secant line:\n",
    "\n",
    "A line that intersects 2 points on a curve. The slope of a curve at a specific point, x1 is best understood as slope of the secant line at increasingly smaller intervals of [x1,x2]. The smaller the difference between x1 and x2, the more precise the secant line approximates the slope at that point on our curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantaneous rate of change:\n",
    "\n",
    "Describes the slope at a particular point. \n",
    "For linear functions, the instantaneous rate of change at any point on the line is the same. For nonlinear function, the instantaneous rate of change describes the slope of the line that's perpendicular to the nonlinear function at a specific point.\n",
    "\n",
    "This line is known as the tangent, and, unlike the secant line, it only intersects our function at one point. So far, we've been working with secant lines that connect 2 points that are increasingly close together. You can think of the tangent line as the secant line when both points are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Limits:\n",
    "A limit desribes the value a function approaches when the input variable to the function approaches a specific value. \n",
    "Defined Limits are whenever the resulting value of a limit is defined at the value the input variable approaches, we say that limit is defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Remove columns that are all nulls\n",
    "-  Remove rows containing missing values for specific columns AND/OR\n",
    "-  Impute (or replace) missing values using a descriptive statistic from the column (NB:: many people instead use a 50% cutoff (if half the values in a column are missing, it's automatically dropped)\n",
    "- Convert categorical features to numeric (if text). the feature transformation process is the same if the numbers used in those categories have no numerical meaning.\n",
    "-  Dummy Coding for the numeric Categorical features (to resolve no numerical meaning to the codes) NB include dropping the original column source for the dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Missing Values ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_null_counts = train.isnull().sum()\n",
    "train_null_counts = train_null_counts[(train_null_counts >0) & (train_null_counts<584)]\n",
    "\n",
    "df_missing_values = train[train_null_counts.index]\n",
    "\n",
    "print(df_missing_values.isnull().sum())\n",
    "print(df_missing_values.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select float columns from the df with missing values\n",
    "float_cols = df_missing_values.select_dtypes(include=['float'])\n",
    "\n",
    "# Return a data frame with missing values replaced with mean of that column.\n",
    "float_cols = float_cols.fillna(float_cols.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Categorical Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = df_no_mv.select_dtypes(include=['object']).columns\n",
    "\n",
    "#show how many categorical values there are for each column.\n",
    "for col in text_cols:\n",
    "    print(col+\":\", len(train[col].unique()))\n",
    "    \n",
    "    # Convert all of the text columns in train to the categorical data type.\n",
    "    train[col] = train[col].astype('category')\n",
    "    \n",
    "train['Utilities'].cat.codes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the categorical variables:\n",
    "This involves assigning a number to each category label, then converting all of the labels in a column to the corresponding numbers.\n",
    "\n",
    "One strategy is to convert the columns to a categorical type. Under this approach, pandas will display the labels as strings, but internally store them as numbers so we can do computations with them. The numbers aren't always compatible with other libraries like Scikit-learn, though, so it's easier to just do the conversion to numeric upfront. We can use the pandas.Categorical() class from pandas to perform the conversion to numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"workclass\",\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical(income[name])\n",
    "    income[name] = col.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling Data:\n",
    "\n",
    "Within scikit-learn, the preprocessing.minmax_scale() function allows us to quickly and easily rescale our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "columns = [\"column one\", \"column two\"]\n",
    "data[columns] = minmax_scale(data[columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Coefficient of Columns:\n",
    "\n",
    "In order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the survival of each passenger. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.\n",
    "\n",
    "The scikit-learn LogisticRegression class has an attribute in which coefficients are stored after the model is fit, LogisticRegression.coef_. We first need to train our model, after which we can access this attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [‘….]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train[columns], train[\"Survived\"])\n",
    "coefficients = lr.coef_\n",
    "\n",
    "#To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:\n",
    "feature_importance = pd.Series(coefficients[0],index=train[columns].columns)\n",
    "\n",
    "feature_importance.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning (Feature Engineering Method):\n",
    "\n",
    "Binning is when you take a continuous feature, like the fare a passenger paid for their ticket, and separate it out into several ranges (or 'bins'), turning it into a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fare(df, cut_points, label_names):\n",
    "    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n",
    "    return df\n",
    "\n",
    "cut_points = [0,12,50,100,1000]\n",
    "label_names = ['0-12','12-50','50-100','100+']\n",
    "    \n",
    "train = process_fare(train, cut_points, label_names)\n",
    "holdout = process_fare(holdout, cut_points, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove ordered relationship:\n",
    "\n",
    "Don't imply any numeric relationship where there isn't one. If we think of the values in the Pclass column, we know they are 1, 2, and 3\n",
    "Class 2 isn't \"worth\" double what class 1 is, and class 3 isn't \"worth\" triple what class 1 is. In order to remove this relationship, we can create dummy columns for each unique value in Pclass:\n",
    "\n",
    "￼\n",
    "Rather than doing this manually, we can use the pandas.get_dummies() function, which will generate columns shown in the diagram above.\n",
    "The following code creates a function to create the dummy columns for the Pclass column and add it back to the original dataframe. It then applies that function the train and test dataframes.\n",
    "\n",
    "NOTE: remember to remove one of each of our dummy variables to reduce the collinearity in each…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df,column_name):\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df\n",
    "\n",
    "train = create_dummies(train,\"Pclass\")\n",
    "test = create_dummies(test,\"Pclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from text columns (Feature Engineering Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First character from a column:\n",
    "train[\"Cabin\"].str[0]\n",
    "\n",
    "# Extract titles from full name:\n",
    "\n",
    "titles = {\n",
    "    \"Mme\":         \"Mrs\",\n",
    "    \"Ms\":          \"Mrs\",\n",
    "    \"Mrs\" :        \"Mrs\",\n",
    "    \"Countess\":    \"Royalty\",\n",
    "    \"Lady\" :       \"Royalty\"\n",
    "}\n",
    "\n",
    "# use extract and regex to get titles out of full name e.g.. Beesley, Mr. Lawrence\n",
    "extracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# map the extracted titles against the predefined dictionary\n",
    "train[\"Title\"] = extracted_titles.map(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing highly correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Correlating Feature Columns With Target Column ##\n",
    "\n",
    "train_subset = train[full_cols_series.index]\n",
    "\n",
    "results = train_subset.corr()\n",
    "print(results)\n",
    "sorted_corrs = abs(results['SalePrice']).sort_values()\n",
    "\n",
    "\n",
    "## 3. Correlation Matrix Heatmap ##\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the columns with a strong correlation from our list\n",
    "strong_corrs = sorted_corrs[sorted_corrs > 0.3]\n",
    "\n",
    "# use get the correlation of these columns with each other from the orginal dataset\n",
    "corrmat = train_subset[strong_corrs.index].corr()\n",
    "print(corrmat)\n",
    "\n",
    "# visualise on a heatmap\n",
    "sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity:\n",
    "\n",
    "Occurs where more than one feature contains data that are similar.\n",
    "\n",
    "The effect of collinearity is that your model will overfit - you may get great results on your test data set, but then the model performs worse on unseen data (like the holdout set).\n",
    "One easy way to understand collinearity is with a simple binary variable like the Sex column in our dataset. Every passenger in our data is categorized as either male or female, so 'not male' is exactly the same as 'female'.\n",
    "As a result, when we created our two dummy columns from the categorical Sex column, we've actually created two columns with identical data in them.\n",
    "\n",
    "**To check for Collinearity:**\n",
    "\n",
    "using the DataFrame.corr() method to produce a correlation matrix, and then use the Seaborn library's seaborn.heatmap() function to plot the values agains each other, \n",
    "The darker squares, whether the darker red or darker blue, indicate pairs of columns that have higher correlation and may lead to collinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "correlations = train.corr()\n",
    "sns.heatmap(correlations)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Nicer version\n",
    "def plot_correlation_heatmap(df):\n",
    "\n",
    "    corr = df.corr()\n",
    "    \n",
    "    sns.set(style=\"white\")\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination with cross-validation (feature selection method):\n",
    "\n",
    "We will be using the feature_selection.RFECV class which performs recursive feature elimination with cross-validation.\n",
    "\n",
    "The RFECV class starts by training a model using all of your features and scores it using cross validation. It then uses the logit coefficients to eliminate the least important feature, and trains and scores a new model. At the end, the class looks at all the scores, and selects the set of features which scored highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "selector = RFECV(lr,cv=10)\n",
    "selector.fit(columns,target column)\n",
    "\n",
    "# Once the RFECV object has been fit, we can use the RFECV.support_ attribute to access a boolean mask of True and False values \n",
    "# which we can use to generate a list of optimized columns:\n",
    "\n",
    "optimized_columns = all_X.columns[selector.support_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## Regression Models\n",
    "\n",
    "Any machine learning model that helps us predict numerical values\n",
    "\n",
    "## Classification Models\n",
    "\n",
    "Where we're trying to predict a label from a fixed set of labels (e.g. blood type or gender).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric machine learning approaches\n",
    "like linear regression and logistic regression. Unlike the k-nearest neighbors algorithm, the result of the training process for these machine learning algorithms is a mathematical function that best approximates the patterns in the training set. In machine learning, this function is often referred to as a model.\n",
    "\n",
    "## Linear Regression Model:\n",
    "\n",
    "most commonly used machine learning model. The logistic regression algorithm works by calculating linear relationships between the features and the target variable and using those to make predictions. Let's look at an algorithm that makes predictions using a different method.\n",
    "\n",
    "Linear regression works well when the target column we're trying to predict, the dependent variable, is ordered and continuous. If the target column instead contains discrete values, then linear regression isn't a good fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#remove the feature we found to have a variance of <0.015\n",
    "features = features.drop('Open Porch SF')\n",
    "\n",
    "target = 'SalePrice'\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[features],train[target])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train[target])\n",
    "test_mse = mean_squared_error(test_predictions, clean_test[target])\n",
    "\n",
    "train_rmse_2 = np.sqrt(train_mse)\n",
    "test_rmse_2 = np.sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual sum of squares \n",
    "\n",
    "To find the optimal parameters for a linear regression model, we want to optimize the model's residual sum of squares (or RSS). If you call, residual (often referred to as errors) describes the difference between the predicted values for the target column (y ^) and the true values (y). We want this difference to be as small as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "If the target column instead contains discrete values, then linear regression isn't a good fit; classification problems. In classification, our target column has a finite set of possible values which represent different categories a row can belong to. We use integers to represent the different categories so we can continue to use mathematical functions to describe how the independent variables map to the dependent variable. Here are a few examples of classification problems:\n",
    "\n",
    "￼\n",
    "While a linear regression model outputs a real number as the label, a logistic regression model outputs a probability value. In binary classification, if the probability value is larger than a certain threshold probability, we assign the label for that row to 1 or 0 otherwise.\n",
    "\n",
    "Logistic regression is really just an adapted version of linear regression for classification problems. Both logistic and linear regression are used to capture linear relationships between the independent variables and the dependent variable.\n",
    "\n",
    "\n",
    "To return the predicted probability, use the predict_proba method. The only required parameter for this method is the num_features by num_sample matrix of observations we want scikit-learn to return predicted probabilities for. For each input row, scikit-learn will return a NumPy array with 2 probability values:\n",
    "* the probability that the row should be labelled 0,\n",
    "* the probability that the row should be labelled 1.\n",
    "\n",
    "Since 0 and 1 are the only 2 possible categories and represent the entire outcome space, these 2 probabilities will always add upto 1.\n",
    "\n",
    "\tprobabilities = logistic_model.predict_proba(admissions[[\"gpa\"]])\n",
    "\t# Probability that the row belongs to label `0`.\n",
    "\tprobabilities[:,0]\n",
    "\t# Probabililty that the row belongs to label `1`.\n",
    "\tprobabilities[:,1]\n",
    "\n",
    "use the predict method to return the label predictions for each row in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28d4c078a055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mcoefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "columns = ['Age_categories_Missing', 'Age_categories_Infant',\n",
    "       'Age_categories_Child', 'Age_categories_Teenager',\n",
    "       'Age_categories_Young Adult', 'Age_categories_Adult',\n",
    "       'Age_categories_Senior', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
    "       'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
    "       'SibSp_scaled', 'Parch_scaled', 'Fare_scaled']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train[columns], train[\"Survived\"])\n",
    "coefficients = lr.coef_\n",
    "feature_importance = pd.Series(coefficients[0],index=train[columns].columns)\n",
    "\n",
    "feature_importance.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests Model:\n",
    "\n",
    "is a specific type of decision tree algorithm. Decision tree algorithms attempt to build the most efficient decision tree based on the training data, and then use that tree to make future predictions. If you'd like to learn about decision trees and random forests in detail, you should check out our decision trees course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors Model:\n",
    " \n",
    "The k-nearest neighbors algorithm finds the observations in our training set most similar to the observation in our test set, and uses the average outcome of those 'neighbor' observations to make a prediction. The 'k' is the number of neighbor observations used to make the prediction.  K-nearest neighbors is known as an instance-based learning algorithm because it relies completely on previous instances to make predictions. The k-nearest neighbors algorithm doesn't try to understand or capture the relationship between the feature columns and the target column. Because the entire training dataset is used to find a new instance's nearest neighbors to make label predictions, this algorithm doesn't scale well to medium and larger datasets\n",
    "\n",
    "Finding the best K value (also known as Grid Search):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = dict()\n",
    "\n",
    "for k in range(1,50,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn,all_X,all_y,cv=10)\n",
    "    knn_scores[k] = np.mean(scores)\n",
    "    \n",
    "print(knn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inbuilt Grid Search:\n",
    "\n",
    "train a number of models across a 'grid' of values and then searched for the model that gave us the highest accuracy.\n",
    "\n",
    "Scikit-learn has a class to perform grid search, model_selection.GridSearchCV(). The 'CV' in the name indicates that we're performing both grid search and cross validation at the same time.\n",
    "By creating a dictionary of parameters and possible values and passing it to the GridSearchCV object you can automate the process. Here's what the code from the previous screen would look like, when implemented using the GridSearchCV class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "hyperparameters = {\n",
    "                   \"n_neighbors\": range(1,50,2)\n",
    "                    }\n",
    "grid = GridSearchCV(knn, param_grid=hyperparameters, cv=10)\n",
    "grid.fit(all_X, all_y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Get the best estimator ie. with the best params/score:\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Running this code will produce the following output:\n",
    " \n",
    "#   {'n_neighbors': 19}\n",
    "#   0.82379349046\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classification algorithm.:\n",
    "\n",
    "A Naive Bayes classifier works by figuring out how likely data attributes are to be associated with a certain class. Let's say we still have one classification -- whether or not you were tired. And let's say we have two data points -- whether or not you ran, and whether or not you woke up early. Bayes' theorem doesn't work in this case, because we have two data points, instead of just one.\n",
    "\n",
    "This is where Naive Bayes can help. Naive Bayes extends Bayes' theorem to handle this case by assuming that each data point is independent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate counts from text using a vectorizer  \n",
    "# We can choose from other available vectorizers, and set many different options\n",
    "# This code performs our step of computing word counts\n",
    "vectorizer = CountVectorizer(stop_words='english', max_df=.05)\n",
    "train_features = vectorizer.fit_transform([r[0] for r in reviews])\n",
    "test_features = vectorizer.transform([r[0] for r in test])\n",
    "\n",
    "# Fit a Naive Bayes model to the training data\n",
    "# This will train the model using the word counts we computed and the existing classifications in the training set\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_features, [int(r[1]) for r in reviews])\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features\n",
    "predictions = nb.predict(test_features)\n",
    "\n",
    "# Compute the error\n",
    "# It's slightly different from our model because the internals of this process work differently from our implementation\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "\n",
    "print(\"Multinomal naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing:\n",
    "Tokens that only occur once don't add anything to the model's prediction power, and removing them will make our algorithm run much more quickly.\n",
    "\n",
    "There are two kinds of features that will reduce prediction accuracy. Features that occur only a few times will cause overfitting, because the model doesn't have enough information to accurately decide whether they're important. These features will probably correlate differently with upvotes in the test set and the training set.\n",
    "\n",
    "Features that occur too many times can also cause issues. These are words like and and to, which occur in nearly every headline. These words don't add any information, because they don't necessarily correlate with upvotes. These types of words are sometimes called stopwords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Overview of the Data ##\n",
    "\n",
    "import pandas as pd\n",
    "submissions = pd.read_csv(\"sel_hn_stories.csv\")\n",
    "submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]\n",
    "submissions = submissions.dropna()\n",
    "\n",
    "## 3. Tokenizing the Headlines ##\n",
    "\n",
    "tokenized_headlines = []\n",
    "\n",
    "for headline in submissions['headline']:\n",
    "    bag = []\n",
    "    bag = headline.split(\" \")\n",
    "    tokenized_headlines.append(bag)\n",
    "\n",
    "print(tokenized_headlines[3])\n",
    "\n",
    "## 4. Preprocessing Tokens to Increase Accuracy ##\n",
    "\n",
    "punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "\n",
    "for bag in tokenized_headlines:   \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in bag:\n",
    "        token = token.lower()\n",
    "        \n",
    "        for char in punctuation:\n",
    "            token = token.replace(char,\"\")\n",
    "            \n",
    "        clean_tokens.append(token)\n",
    "     \n",
    "    clean_tokenized.append(clean_tokens)   \n",
    "\n",
    "## 5. Assembling a Matrix of Unique Words ##\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "\n",
    "for bag in clean_tokenized:\n",
    "    for token in bag:\n",
    "        if token in single_tokens:\n",
    "            # word occurs more than once, add it to the real list\n",
    "            unique_tokens.append(token)\n",
    "        else:\n",
    "            # first time we've found the word, add it to the unique_tokens list\n",
    "            single_tokens.append(token)\n",
    "            \n",
    "# Create a data frame with 0 counts against each of the unique words as columns:\n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)\n",
    "\n",
    "## 6. Counting Token Occurrences ##\n",
    "for idx, bag in enumerate(clean_tokenized):\n",
    "    for token in bag:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[idx][token] += 1 \n",
    "        \n",
    "\n",
    "## 7. Removing Columns to Increase Accuracy ##\n",
    "# To reduce the number of features and enable the linear regression model to make better predictions, \n",
    "# we'll remove any words that occur fewer than 5 times or more than 100 times;\n",
    "word_counts = counts.sum(axis=0)\n",
    "counts = counts.loc[:,(word_counts >= 5) & (word_counts <= 100)]\n",
    "\n",
    "## 8. Splitting the Data Into Train and Test Sets ##\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)\n",
    "\n",
    "## 10. Calculating Prediction Error ##\n",
    "mse = (((predictions-y_test)**2).sum())/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "We can use trees for classification or regression problems. enables us to automatically construct a decision tree that tells us what outcomes we should predict in certain situations.\n",
    "The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome. One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't.\n",
    "\n",
    "Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables. This involves assigning a number to each category label, then converting all of the labels in a column to the corresponding numbers.\n",
    "\n",
    "One strategy is to convert the columns to a categorical type. Under this approach, pandas will display the labels as strings, but internally store them as numbers so we can do computations with them. The numbers aren't always compatible with other libraries like Scikit-learn, though, so it's easier to just do the conversion to numeric upfront. We can use the pandas.Categorical() class from pandas to perform the conversion to numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"workclass\",\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical(income[name])\n",
    "    income[name] = col.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to continue splitting nodes until we get to a point where all of the rows in a node have the same value for target column\n",
    "\n",
    "We use the DecisionTreeClassifier class for classification problems, and DecisionTreeRegressor for regression problems. The sklearn.tree package includes both of these classes.\n",
    "In this case, we're predicting a binary outcome, so we'll use a classifier.\n",
    "\n",
    "Our test set accuracy decreases to .691, and our training set accuracy increases to .975.\n",
    "One way to prevent overfitting is to block the tree from growing beyond a certain depth (we tried this before). Another technique is called pruning. Pruning involves building a full tree, and then removing the leaves that don't add to prediction accuracy. Pruning prevents a model from becoming overly complex. It can result in a simpler model that has higher accuracy on the testing set.\n",
    "Data scientists use pruning less often than parameter optimization (what we just did) and ensembling. It's still an important technique, though, and we'll cover it in more depth down the line.\n",
    "\n",
    "Let's go over the main advantages and disadvantages of using decision trees. The main advantages of using decision trees is that they're:\n",
    "* Easy to interpret\n",
    "* Relatively fast to fit and make predictions\n",
    "* Able to handle multiple types of data\n",
    "* Able to pick up nonlinearities in data, and usually fairly accurate\n",
    "The main disadvantage of using decision trees is their tendency to overfit.\n",
    "Decision trees are a good choice for tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing.\n",
    "The most powerful way to reduce decision tree overfitting is to create ensembles of trees. The random forest algorithm is a popular choice for doing this. In cases where prediction accuracy is the most important consideration, random forests usually perform better.\n",
    "\n",
    "The most powerful tool for reducing decision tree overfitting is called the random forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "\n",
    "#compute error with ROC AUC\n",
    "error = roc_auc_score(test['high_income'], predictions)\n",
    "print(error)\n",
    "\n",
    "## 5. Computing Error on the Training Set ##\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(train['high_income'],predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction\n",
    "\n",
    "The models are approaching the same problem in slightly different ways, and building different trees because we used different parameters for each one. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches. The more \"diverse\" or dissimilar the models we use to construct an ensemble are, the stronger their combined predictions will be (assuming that all of the models have about the same accuracy). Ensembling a decision tree and a logistic regression model, for example, will result in stronger predictions than ensembling two decision trees with similar parameters. That's because those two models use very different approaches to arrive at their answers.\n",
    "\n",
    "A random forest is an ensemble of decision trees. If we don't make any modifications to the trees, each tree will be exactly the same, so we'll get no boost when we ensemble them. In order to make ensembling effective, we have to introduce variation into each individual decision tree model.\n",
    "If we introduce variation, each tree will be be constructed slightly differently, and will therefore make different predictions. This variation is what puts the \"random\" in \"random forest.\"\n",
    "\n",
    "Similar to decision trees, we can tweak some of the parameters for random forests, including:\n",
    "* min_samples_leaf\n",
    "* min_samples_split\n",
    "* max_depth\n",
    "* max_leaf_nodes\n",
    "These parameters apply to the individual trees in the model, and change how they are constructed. There are also parameters specific to the random forest that alter its overall construction:\n",
    "* n_estimators\n",
    "* bootstrap - \"Bootstrap aggregation\" is another name for bagging; this parameter indicates whether to turn it on (Defaults to True)\n",
    "Refer to the documentation for a full list of parameters.\n",
    "Tweaking parameters can increase the accuracy of the forest. The easiest tweak is to increase the number of estimators we use. This approach yields diminishing returns -- going from 10 trees to 100 will make a bigger difference than going from 100 to 500, which will make a bigger difference than going from 500 to 1000. The accuracy increase function is logarithmic, so increasing the number of trees beyond a certain number (usually 200) won't help much at all.\n",
    "\n",
    "\n",
    "One of the major advantages of random forests over single decision trees is that they tend to overfit less. The average of 100 or more trees will be more likely to hone in on the signal and ignore the noise. The signal will be the same across all of the trees, whereas each tree will hone in on the noise differently. This means that the average will discard the noise and keep the signal.\n",
    "\n",
    "The main weaknesses of using a random forest are:\n",
    "* They're difficult to interpret - Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "* They take longer to create - Making two trees takes twice as long as making one, making three takes three times as long, and so on. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier. We'll discuss parallelization in greater detail later on.\n",
    "Given these trade-offs, it makes sense to use random forests in situations where accuracy is of the utmost importance; being able to interpret or explain the decisions the model is making isn't key. In cases where time is of the essence or interpretability is important, a single decision tree may be a better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "\n",
    "print(roc_auc_score(test[\"high_income\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
