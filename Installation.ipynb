{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 3 & Anaconda\n",
    "\n",
    "To install Python 3 and the data science libraries we can use with it, we'll use a package manager named Anaconda. \n",
    "To test your installation, type python3 and press Enter. This enters interactive mode.\n",
    "\n",
    "\n",
    "The >>>symbols at the start of the line indicate that you're now in interactive mode. This mode allows you to write a line of Python code and press Enter to see the output. Try it out!\n",
    "* Type a = 1\n",
    "* Press Enter\n",
    "* Type print(a)\n",
    "* Press Enter\n",
    "type exit() and press the Enter key to exit interactive mode.\n",
    "\n",
    "\n",
    "To verify that the Anaconda package manager has installed properly, type conda in your command line environment. You should see output information on the library. If you don't see the expected output, use StackOverflow and our Slack community to get help.\n",
    "\n",
    "Anaconda comes with several packages for scientific computing. You can see the full list of libraries here. For example, it automatically installs libraries like pandas and Matplotlib. You can verify that you have them by launching interactive mode (using the command python3) and importing the libraries.\n",
    "\n",
    "Anaconda packages for Mac0sX https://docs.anaconda.com/anaconda/packages/py3.6_osx-64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplot Libraries\n",
    "\n",
    "https://matplotlib.org/api/pyplot_summary.html\n",
    "\n",
    "Installing matplotlib using Anaconda: \n",
    "\n",
    "\tconda install matplotlib\n",
    "\n",
    "We recommend working with matplotlib using Jupyter Notebook because it can render the plots in the notebook itself. \n",
    "You will need to run the following Jupyter magic in a code cell each time you open your notebook: \n",
    "\t\n",
    "\t%matplotlib inline. \n",
    "\n",
    "\n",
    "Instead of using a print statement, you can also add a line to the end of a cell that just refers to the variable name and it will be displayed.\n",
    "write some code in a code cell, then press Shift + Enter to see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basemap\n",
    "\n",
    "https://matplotlib.org/basemap/users/intro.html\n",
    "\n",
    "The easiest way to install basemap is through Anaconda. If you're new to Anaconda, we recommend checking out our Python and Pandas installation project:\n",
    "\t\n",
    "\tconda install basemap\n",
    "\n",
    "The Basemap library has some external dependencies that Anaconda handles the installation for. To test the installation, run the following import code:\n",
    "\t\n",
    "\tfrom mpl_toolkits.basemap import Basemap\n",
    "\n",
    "If an error is returned, we recommend searching for similar errors on StackOverflow to help debug the issue. Because basemap uses matplotlib, you'll want to import matplotlib.pyplot into your environment when you use Basemap.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh\n",
    "\n",
    "http://bokeh.pydata.org/en/latest/\n",
    "\n",
    "There are multiple ways to install Bokeh, and we recommend the easiest one, which is to use the Anaconda Python distribution and enter this command at a Bash or Windows command prompt:\n",
    "    \n",
    "    conda install bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "The source code for the PySpark library is located in the python/pyspark directory, but the executable version of the library is located in bin/pyspark. To test whether your installation built Spark properly, run the command bin/pyspark to start up the PySpark shell. \n",
    "\n",
    "While the output is verbose, you can see that the shell automatically initialized the SparkContext object and assigned it to the variable sc.\n",
    "\n",
    "You don't have to run bin/pyspark from the folder that contains it. Because it's in your home directory, you can use ~/spark-1.6.1-bin-hadoop2.6/bin/pyspark to launch the PySpark shell from other directories on your machine(Note: replace 1.6.1 with 1.6.2 for newer version users). This way, you can switch to the directory that contains the data you want to use, launch the PySpark shell, and read the data in without having to use its full path. The folder you're in when you launch the PySpark shell will be the local context for working with files in Spark.\n",
    "\n",
    "\n",
    "Requires Java SDK v. 1.7 or later (check with java -version)\n",
    "\n",
    "We'll download and work with a pre-built version of Spark instead. Navigate to the Spark downloads page and select the following options:\n",
    "1. 1.6.2\n",
    "2. Pre-built for Hadoop 2.6\n",
    "3. Direct Download\n",
    "\n",
    "You can make your Jupyter Notebook application aware of Spark in a few different ways. One is to create a configuration file and launch Jupyter Notebook with that configuration. Another is to import PySpark at runtime. We'll focus on the latter approach, so you won't have to restart Jupyter Notebook each time you want to use Spark.\n",
    "\n",
    "First, you'll need to copy the full path to the pre-built Spark folder and set it as a shell environment variable. This way, you can specify Spark's location a single time, and every Python program you write will have access to it. If you move the Spark folder, you can change the path specification once and your code will work just fine.\n",
    "\n",
    "Mac / Linux\n",
    "\n",
    "Use nano or another text editor to open your shell environment's configuration file. If you're using the default Terminal application, the file should be in ~/.bash_profile . If you're using ZSH instead, your configuration file will be in ~/.zshrc.\n",
    "Add the following line to the end of the file, replacing {full path to Spark} with the actual path to Spark:\n",
    "\n",
    "    export SPARK_HOME=\"{full path to Spark, eg /users/home/jeff/spark-2.0.1-bin-hadoop2.7/}\"\n",
    "\n",
    "Exit the text editor and run either source ~/.bash_profile or source ~/.zshrc so the shell reads in and applies the update you made.\n",
    "Windows\n",
    "\n",
    "If you've never added environment variables, read this tutorial before you proceed.\n",
    "\n",
    "Set the SPARK_HOME environment variable to the full path of the Spark folder (e.g. c:/Users/Jeff/spark-2.0.1-bin-hadoop2.7/).\n",
    "Next, let's install the findspark Python library, which looks up the location of PySpark using the environment variable we just set. Use pip to install the findspark library:\n",
    "\n",
    "    pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
