{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark & MapReduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop:\n",
    "\n",
    "Hadoop consists of a file system (Hadoop Distributed File System, or HDFS) and its own implementation of the MapReduce paradigm. MapReduce converts computations into Map and Reduce steps that Hadoop can easily distribute over many machines. We'll cover how MapReduce works in greater depth later in this lesson.\n",
    "\n",
    "Hadoop made it possible to analyze large data sets, but relied heavily on disk storage (rather than memory) for computation. While it's inexpensive to store large volumes of data this way, it makes accessing and processing it much slower.\n",
    "\n",
    "\n",
    "Hadoop wasn't a great solution for calculations requiring multiple passes over the same data or many intermediate steps, due to the need to write to and read from the disk between each step. This drawback also made Hadoop difficult to use for interactive data analysis, the main task data scientists need to do.\n",
    "\n",
    "\n",
    "Hadoop also suffered from suboptimal support for the additional libraries many data scientists needed, such as SQL and machine learning implementations. Once the cost of RAM (computer memory) started to drop significantly, augmenting or replacing Hadoop by storing data in-memory quickly emerged as an appealing alternative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark:\n",
    "\n",
    "The UC Berkeley AMP Lab spearheaded groundbreaking work to develop Spark, which uses distributed, in-memory data structures to improve speeds for many data processing workloads by several orders of magnitude. \n",
    "\n",
    "## PySpark:\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called PySpark that allows us to interface with RDDs in Python. Thanks to a library called Py4J, Python can interface with Java objects (in our case RDDs). Py4J is also one of the tools that makes PySpark work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d0439f26493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Find path to PySpark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import PySpark and initialize SparkContext object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mspark_home\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/findspark.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[1;32m     33\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "# Find path to PySpark.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark and initialize SparkContext object.\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Read `recent-grads.csv` in to an RDD.\n",
    "f = sc.textFile('data/recent-grads.csv')\n",
    "data = f.map(lambda line: line.split('\\n'))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "The core data structure in Spark is a resilient distributed data set (RDD). As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accesible to that object.\n",
    "\n",
    "\n",
    "Spark's RDD implementation also lets us evaluate code \"lazily,\" meaning we can postpone running a calculation until absolutely necessary. eg. Spark created a pointer to the file, but didn't actually read it into raw_data until raw_data.take(5) needed that variable to run its logic. The advantage of \"lazy\" evaluation is that we can build up a queue of tasks and let Spark optimize the overall workflow in the background. In regular Python, the interpreter can't do much workflow optimization\n",
    "\n",
    "\n",
    "RDD objects are **immutable**, meaning that we can't change their values once we've created them. In Python, list and dictionary objects are mutable (we can change their values), while tuple objects are immutable. The only way to modify a tuple object in Python is to create a new tuple object with the necessary updates. Spark uses the immutability of RDDs to enhance calculation speeds. T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Every operation or calculation in Spark is essentially a series of steps that we can chain together and run in succession to form a **pipeline**. Each step in the pipeline returns either a Python value (such as an integer), a Python data structure (such as a dictionary), or an RDD object. \n",
    "\n",
    "In the following code cell, we'll filter out actors for whom the profession is blank, lowercase each profession, generate a histogram of professions, and output the first five tuples in the histogram.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1)) \\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "Two types of methods in Spark:\n",
    "\n",
    "\t1. Transformations - map(), reduceByKey()\n",
    "\t2. Actions - take(), reduce(), saveAsTextFile(), collect()\n",
    "\n",
    "\n",
    "NOTE: Even though Spark simplifies chaining lots of transformations together, it's good practice to use actions to observe the intermediate RDD objects between those transformations. This will let you know whether your transformations are working the way you expect them to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "are lazy operations that always return a reference to an RDD object. Spark doesn't actually run the transformations, though, until an action needs to use the RDD resulting from a transformation. \n",
    "\n",
    "Any function that returns an RDD is a transformation, and any function that returns a value is an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_hamlet = sc.textFile('hamlet.txt')\n",
    "#= f.map(lambda line: line.split('\\n'))\n",
    "raw_hamlet.take(5)\n",
    "\n",
    "## 2. The Map Method ##\n",
    "\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "\n",
    "## 4. The FlatMap Method ##\n",
    "\n",
    "def hamlet_speaks(line):\n",
    "    id = line[0]\n",
    "    speaketh = False\n",
    "    \n",
    "    if \"HAMLET\" in line:\n",
    "        speaketh = True\n",
    "    \n",
    "    if speaketh:\n",
    "        yield id,\"hamlet speaketh!\"\n",
    "\n",
    "hamlet_spoken = split_hamlet.flatMap(lambda x: hamlet_speaks(x))\n",
    "hamlet_spoken.take(10)\n",
    "\n",
    "## 5. Filter Using a Named Function ##\n",
    "\n",
    "def filter_hamlet_speaks(line):\n",
    "    \n",
    "    if 'HAMLET' in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "hamlet_spoken_lines = split_hamlet.filter(lambda line: filter_hamlet_speaks(line))\n",
    "hamlet_spoken_lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Spark forces the evaluation of lazy code. If we only chain together transformation methods and print the resulting RDD object, we'll see the type of RDD (e.g. a PythonRDD or PipelinedRDD object), but not the elements within it. That's because the computation hasn't actually happened yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_count = 0\n",
    "spoken_101 = list()\n",
    "spoken_count = hamlet_spoken_lines.count()\n",
    "spoken_collect = hamlet_spoken_lines.collect()\n",
    "spoken_101 = spoken_collect[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext Object:\n",
    "\n",
    "the SparkContext object manages the connection to the clusters, and coordinates the running of processes on those clusters. More specifically, it connects to the cluster managers. The cluster managers control the executors that run the computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"daily_show.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print first 5 elements of RDD\n",
    "The RDD object raw_data closely resembles a list of string objects, with one object for each line in the data set.\n",
    "\n",
    "We then use the take() method to print the first five elements of the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Function:\n",
    "\n",
    "The map(f) function applies the function f to every element in the RDD. Because RDDs are iterable objects (like most Python objects), Spark runs function f on each iteration and returns a new RDD.\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to each line\n",
    "hamlet_text_only = hamlet_filter_lines.map(lambda line: [item for item in line if item !=''])\n",
    "\n",
    "# Remove any list items that only contain the pipe character (|), and replace any pipe characters that appear within strings with an empty character.\n",
    "clean_hamlet = hamlet_text_only.map(lambda x: [word.replace('|', '') for word in x if word != '|'])\n",
    "\n",
    "# Find out how many elements are in the collection:\n",
    "print(tally.take(tally.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey:\n",
    "\n",
    "reduceByKey(f) combines tuples with the same key using the function we specify, f.\n",
    "\n",
    "\t('YEAR', 1)\n",
    "\t('1991', 1)\n",
    "\t('1991', 1)\n",
    "\t('1991', 1)\n",
    "\t('1991', 1)\n",
    "\n",
    "We'd like to reduce that down to:\n",
    "\n",
    "\t('YEAR', 1)\n",
    "\t('1991', 4)\n",
    "\t...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter an RDD:\n",
    "\n",
    "Spark comes with a filter(f) function that creates a new RDD by filtering an existing one for specific criteria. If we specify a function f that returns a binary value, True or False, the resulting RDD will consist of elements where the function evaluated to True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_daily_show = daily_show.filter(lambda line: line[0] != 'YEAR')\n",
    "\n",
    "# or:\n",
    "\n",
    "hamlet_filter_lines = hamlet_with_ids.filter(lambda line: len(line)>1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames:\n",
    "\n",
    "Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change. \n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are built around Series objects, while Spark DataFrames are built around RDDs. \n",
    "\n",
    "We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different.\n",
    "\n",
    "Other shared df methods include:\n",
    "\n",
    "\tagg()\n",
    "\tjoin()\n",
    "\tsort()\n",
    "\twhere()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b414e671290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Pass in the SparkContext object `sc`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msqlCtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read JSON data into a DataFrame object `df`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"data/census_2010.json\")\n",
    "\n",
    "#Display the inferred Schema:\n",
    "df.printSchema()\n",
    "\n",
    "#Show n rows:\n",
    "df.show(5)\n",
    "\n",
    "# return rows:\n",
    "row_one = df.head(5)[0]\n",
    "\n",
    "# Access value for age\n",
    "row_one.age\n",
    "\n",
    "# Access the first value\n",
    "row_one[0]\n",
    "\n",
    "# Selecting Columns:\n",
    "df.select('age','males','females').show()\n",
    "# or\n",
    "df[['age','males','females']].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Schema Results in:\n",
    "\n",
    "\troot\n",
    "\t |-- age: long (nullable = true)\n",
    "\t |-- females: long (nullable = true)\n",
    "\t |-- males: long (nullable = true)\n",
    "\t |-- total: long (nullable = true)\n",
    "\t |-- year: long (nullable = true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pandas Dataframe:\n",
    "\n",
    "convert a Spark DataFrame to a pandas DataFrame using the toPandas() method. Converting an entire Spark DataFrame to a pandas DataFrame works just fine for small data sets. For larger ones, though, we'll want to select a subset of the data that's more manageable for pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df['total'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register a Spark DataFrame as a SQL DB Table:\n",
    "\n",
    "Call the registerTempTable() method on that DataFrame object. This method requires one string parameter, name, that we use to set the table name for reference in our SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "\n",
    "df.registerTempTable('census2010')\n",
    "\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query ='SELECT males, females FROM census2010'\n",
    "df = sqlCtx.sql(query)\n",
    "\n",
    "# then get summary stats on the resulting data frame:\n",
    "df.describe().show()\n",
    "\n",
    "\n",
    "query = \"SELECT age FROM census2010\"\n",
    "sqlCtx.sql(query).show(20)\n",
    "\n",
    "## 4. Filtering ##\n",
    "\n",
    "query = 'SELECT males,females FROM census2010 WHERE age > 5 AND age <15'\n",
    "sqlCtx.sql(query).show()\n",
    "\n",
    "## 5. Mixing Functionality ##\n",
    "\n",
    "query ='SELECT males, females FROM census2010'\n",
    "df = sqlCtx.sql(query)\n",
    "df.describe().show()\n",
    "\n",
    "\n",
    "## 6. Multiple tables ##\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.registerTempTable('census2010')\n",
    "\n",
    "df = sqlCtx.read.json(\"census_1980.json\")\n",
    "df.registerTempTable('census1980')\n",
    "\n",
    "df = sqlCtx.read.json(\"census_1990.json\")\n",
    "df.registerTempTable('census1990')\n",
    "\n",
    "df = sqlCtx.read.json(\"census_2000.json\")\n",
    "df.registerTempTable('census2000')\n",
    "\n",
    "tables = sqlCtx.tableNames()\n",
    "print(tables)\n",
    "\n",
    "## 7. Joins ##\n",
    "\n",
    "query = 'SELECT census2010.total, census2000.total FROM census2010, census2000 WHERE census2010.age =census2000.age'\n",
    "sqlCtx.sql(query).show(20)\n",
    "\n",
    "\n",
    "## 8. SQL Functions ##\n",
    "\n",
    "query = 'SELECT sum(census2010.total), sum(census2000.total), sum(census1990.total)\\\n",
    "         FROM census2010, census2000, census1990 \\\n",
    "         WHERE census2010.age= census2000.age AND \\\n",
    "         census2000.age = census1990.age'\n",
    "            \n",
    "sqlCtx.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Extract Line Numbers ##\n",
    "\n",
    "raw_hamlet = sc.textFile(\"hamlet.txt\")\n",
    "split_hamlet = raw_hamlet.map(lambda line: line.split('\\t'))\n",
    "split_hamlet.take(5)\n",
    "\n",
    "def format_id(line):\n",
    "    \n",
    "    new_line = list()\n",
    "    \n",
    "    for item in line:\n",
    "        \n",
    "        if item == line[0]:\n",
    "            \n",
    "            item = item.split(\"@\")[1]\n",
    "\n",
    "        new_line.append(item)\n",
    "\n",
    "    return new_line\n",
    "\n",
    "hamlet_with_ids = split_hamlet.map(lambda line: format_id(line))\n",
    "hamlet_with_ids.take(5)\n",
    "\n",
    "## 3. Remove Blank Values ##\n",
    "\n",
    "hamlet_with_ids.take(10)\n",
    "\n",
    "hamlet_filter_lines = hamlet_with_ids.filter(lambda line: len(line)>1)\n",
    "hamlet_filter_lines.take(10)\n",
    "\n",
    "def remove_empties(line):\n",
    "    \n",
    "    new_line = list()\n",
    "    for item in line:\n",
    "        if item != \"\":\n",
    "            new_line.append(item)\n",
    "    \n",
    "    return new_line\n",
    "                   \n",
    "hamlet_text_only = hamlet_filter_lines.map(lambda line: [item for item in line if item !=''])\n",
    "hamlet_text_only.take(10)\n",
    "#hamlet_with_ids.take(10)\n",
    "\n",
    "## 4. Remove Pipe Characters ##\n",
    "\n",
    "def clean_pipes(line):\n",
    "    \n",
    "    new_line = []\n",
    "    \n",
    "    for item in line:\n",
    "        \n",
    "        if item !=\"|\":\n",
    "            \n",
    "           item = item.replace(\"|\",\" \")\n",
    "           new_line.append(item)\n",
    "    \n",
    "    return new_line\n",
    "\n",
    "#clean_pipes_hamlet = hamlet_text_only.map(lambda line: clean_pipes(line))\n",
    "#clean_hamlet = clean_pipes_hamlet.filter(lambda line: len(line)>1)\n",
    "\n",
    "\n",
    "clean_hamlet = hamlet_text_only.map(lambda x: [word.replace('|', '') for word in x if word != '|'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
